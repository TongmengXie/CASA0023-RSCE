
@article{schulte_to_buhne_better_2018,
	title = {Better together: {Integrating} and fusing multispectral and radar satellite imagery to inform biodiversity monitoring, ecological research and conservation science},
	volume = {9},
	issn = {2041-210X},
	shorttitle = {Better together},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12942},
	doi = {10.1111/2041-210X.12942},
	abstract = {The availability and accessibility of multispectral and radar satellite remote sensing (SRS) imagery are at an unprecedented high. These data have both become standard source of information for investigating species ecology and ecosystems structure, composition and function at large scales. Since they capture complementary aspects of the Earth's surface, synergies between these two types of imagery have the potential to greatly expand research and monitoring opportunities. However, despite the benefits of combining multispectral and radar SRS data, data fusion techniques, including image fusion, are not commonly used in biodiversity monitoring, ecology and conservation. To help close this application gap, we provide for the first time an overview of the most common SRS data fusion techniques, discussing their benefits and drawbacks, and pull together case studies illustrating the added value for biodiversity research and monitoring. Integrating and fusing multispectral and radar images can significantly improve our ability to assess the distribution as well as the horizontal and vertical structure of ecosystems. Additionally, SRS data fusion has the potential to increase opportunities for mapping species distribution and community composition, as well as for monitoring threats to biodiversity. Uptake of these techniques will benefit from more effective collaboration between remote sensing and biodiversity experts, making the reporting of methodologies more transparent, expanding SRS image processing capacity and promoting widespread open access to satellite imagery. In the context of a global biodiversity crisis, being able to track subtle changes in the biosphere across adequate spatial and temporal extents and resolutions is crucial. By making key parameter estimates derived from SRS data more accurate, SRS data fusion promises to become a powerful tool to help address current monitoring needs, and could support the development of essential biodiversity variables.},
	language = {en},
	number = {4},
	urldate = {2023-02-03},
	journal = {Methods in Ecology and Evolution},
	author = {Schulte to Bühne, Henrike and Pettorelli, Nathalie},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12942},
	keywords = {image fusion, object-level fusion, pixel-level fusion, remote sensing of biodiversity, satellite data fusion},
	pages = {849--865},
	file = {Full Text PDF:D\:\\Zotero\\storage\\KVET6MZ3\\Schulte to Bühne 和 Pettorelli - 2018 - Better together Integrating and fusing multispect.pdf:application/pdf;Snapshot:D\:\\Zotero\\storage\\52KECAZS\\2041-210X.html:text/html},
}

@article{li_assessing_2021,
	title = {Assessing {Surface} {Water} {Flood} {Risks} in {Urban} {Areas} {Using} {Machine} {Learning}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-4441},
	url = {https://www.mdpi.com/2073-4441/13/24/3520},
	doi = {10.3390/w13243520},
	abstract = {Urban flooding is a devastating natural hazard for cities around the world. Flood risk mapping is a key tool in flood management. However, it is computationally expensive to produce flood risk maps using hydrodynamic models. To this end, this paper investigates the use of machine learning for the assessment of surface water flood risks in urban areas. The factors that are considered in machine learning models include coordinates, elevation, slope gradient, imperviousness, land use, land cover, soil type, substrate, distance to river, distance to road, and normalized difference vegetation index. The machine learning models are tested using the case study of Exeter, UK. The performance of machine learning algorithms, including naïve Bayes, perceptron, artificial neural networks (ANNs), and convolutional neural networks (CNNs), is compared based on a spectrum of indicators, e.g., accuracy, F-beta score, and receiver operating characteristic curve. The results obtained from the case study show that the flood risk maps can be accurately generated by the machine learning models. The performance of models on the 30-year flood event is better than 100-year and 1000-year flood events. The CNNs and ANNs outperform the other machine learning algorithms tested. This study shows that machine learning can help provide rapid flood mapping, and contribute to urban flood risk assessment and management.},
	language = {en},
	number = {24},
	urldate = {2023-02-16},
	journal = {Water},
	author = {Li, Zhufeng and Liu, Haixing and Luo, Chunbo and Fu, Guangtao},
	month = jan,
	year = {2021},
	note = {Number: 24
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {machine learning, CNNs, deep learning, flood management, surface water flooding},
	pages = {3520},
	file = {Full Text PDF:D\:\\Zotero\\storage\\WQCBTXMU\\Li 等 - 2021 - Assessing Surface Water Flood Risks in Urban Areas.pdf:application/pdf},
}

@article{nair2018automated,
  title={Automated Image Annotation: A Survey},
  author={Nair, Rahul and Paul, P. J. and Jacob, K. Poulose},
  journal={Journal of Imaging},
  volume={4},
  number={3},
  pages={37},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@misc{robison2018future,
  title={The Future of Image Annotation: Human in the Loop},
  author={Robison, Keela},
  year={2018},
  howpublished={\url{https://lionbridge.ai/articles/the-future-of-image-annotation-human-in-the-loop/}},
  note={Accessed: 28th Feb 2023}
}


@article{schulte_to_buhne_better_2018,
	title = {Better together: {Integrating} and fusing multispectral and radar satellite imagery to inform biodiversity monitoring, ecological research and conservation science},
	volume = {9},
	issn = {2041-210X},
	shorttitle = {Better together},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12942},
	doi = {10.1111/2041-210X.12942},
	abstract = {The availability and accessibility of multispectral and radar satellite remote sensing (SRS) imagery are at an unprecedented high. These data have both become standard source of information for investigating species ecology and ecosystems structure, composition and function at large scales. Since they capture complementary aspects of the Earth's surface, synergies between these two types of imagery have the potential to greatly expand research and monitoring opportunities. However, despite the benefits of combining multispectral and radar SRS data, data fusion techniques, including image fusion, are not commonly used in biodiversity monitoring, ecology and conservation. To help close this application gap, we provide for the first time an overview of the most common SRS data fusion techniques, discussing their benefits and drawbacks, and pull together case studies illustrating the added value for biodiversity research and monitoring. Integrating and fusing multispectral and radar images can significantly improve our ability to assess the distribution as well as the horizontal and vertical structure of ecosystems. Additionally, SRS data fusion has the potential to increase opportunities for mapping species distribution and community composition, as well as for monitoring threats to biodiversity. Uptake of these techniques will benefit from more effective collaboration between remote sensing and biodiversity experts, making the reporting of methodologies more transparent, expanding SRS image processing capacity and promoting widespread open access to satellite imagery. In the context of a global biodiversity crisis, being able to track subtle changes in the biosphere across adequate spatial and temporal extents and resolutions is crucial. By making key parameter estimates derived from SRS data more accurate, SRS data fusion promises to become a powerful tool to help address current monitoring needs, and could support the development of essential biodiversity variables.},
	language = {en},
	number = {4},
	urldate = {2023-02-03},
	journal = {Methods in Ecology and Evolution},
	author = {Schulte to Bühne, Henrike and Pettorelli, Nathalie},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12942},
	keywords = {image fusion, object-level fusion, pixel-level fusion, remote sensing of biodiversity, satellite data fusion},
	pages = {849--865},
	file = {Full Text PDF:D\:\\Zotero\\storage\\KVET6MZ3\\Schulte to Bühne 和 Pettorelli - 2018 - Better together Integrating and fusing multispect.pdf:application/pdf;Snapshot:D\:\\Zotero\\storage\\52KECAZS\\2041-210X.html:text/html},
}

@inproceedings{Jensen1986IntroductoryDI,
  title={Introductory Digital Image Processing: A Remote Sensing Perspective},
  author={J. Robert Jensen},
  year={1986}
}

@article{maclachlan_sustainable_2021,
	title = {Sustainable {City} {Planning}: {A} {Data}-{Driven} {Approach} for {Mitigating} {Urban} {Heat}},
	volume = {6},
	issn = {2297-3362},
	shorttitle = {Sustainable {City} {Planning}},
	url = {https://www.frontiersin.org/articles/10.3389/fbuil.2020.519599},
	abstract = {Urban areas are expected to triple by 2030 in order to accommodate 60\% of the global population. Anthropogenic landscape modifications expand coverage of impervious surfaces inducing the urban heat island (UHI) effect, a critical twenty first century challenge associated with increased economic expenditure, energy consumption, and adverse health impacts. Yet, omission of UHI measures from global climate models and metropolitan planning methodologies precludes effective sustainable development governance. We present an approach that integrates Earth observation and climate data with three-dimensional urban models to determine optimal tree placement (per square meter) within proposed urban developments to enable more effective localized UHI mitigation. Such data-driven planning decisions will enhance the future sustainability of our cities to align with current global urban development agendas.},
	urldate = {2023-03-12},
	journal = {Frontiers in Built Environment},
	author = {MacLachlan, Andrew and Biggs, Eloise and Roberts, Gareth and Boruff, Bryan},
	year = {2021},
	file = {Full Text PDF:D\:\\Zotero\\storage\\EN7FBDZP\\MacLachlan 等 - 2021 - Sustainable City Planning A Data-Driven Approach .pdf:application/pdf},
}



@article{gorelick_google_2017,
	series = {Big {Remotely} {Sensed} {Data}: tools, applications and experiences},
	title = {Google {Earth} {Engine}: {Planetary}-scale geospatial analysis for everyone},
	volume = {202},
	issn = {0034-4257},
	shorttitle = {Google {Earth} {Engine}},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425717302900},
	doi = {10.1016/j.rse.2017.06.031},
	abstract = {Google Earth Engine is a cloud-based platform for planetary-scale geospatial analysis that brings Google's massive computational capabilities to bear on a variety of high-impact societal issues including deforestation, drought, disaster, disease, food security, water management, climate monitoring and environmental protection. It is unique in the field as an integrated platform designed to empower not only traditional remote sensing scientists, but also a much wider audience that lacks the technical capacity needed to utilize traditional supercomputers or large-scale commodity cloud computing resources.},
	language = {en},
	urldate = {2023-03-14},
	journal = {Remote Sensing of Environment},
	author = {Gorelick, Noel and Hancher, Matt and Dixon, Mike and Ilyushchenko, Simon and Thau, David and Moore, Rebecca},
	month = dec,
	year = {2017},
	keywords = {Analysis, Big data, Cloud computing, Data democratization, Earth Engine, Platform},
	pages = {18--27},
	file = {ScienceDirect Full Text PDF:D\:\\Zotero\\storage\\NZ77554A\\Gorelick 等 - 2017 - Google Earth Engine Planetary-scale geospatial an.pdf:application/pdf;ScienceDirect Snapshot:D\:\\Zotero\\storage\\YW6K439E\\S0034425717302900.html:text/html},
}

@misc{google_machine_2023,
	title = {Machine {Learning} in {Earth} {Engine} {\textbar} {Google} {Earth} {Engine}},
	url = {https://developers.google.com/earth-engine/guides/machine-learning},
  author = {Google},
	year = {2023},
	language = {en},
	urldate = {2023-03-14},
	journal = {Google Developers},
	file = {Snapshot:D\:\\Zotero\\storage\\JU5WUHCP\\machine-learning.html:text/html},
}

@misc{google_reducer_2023,
	title = {Reducer {Overview} {\textbar} {Google} {Earth} {Engine}},
	url = {https://developers.google.com/earth-engine/guides/reducers_intro},
	language = {en},
	author = {Google},
	year = {2023},
	urldate = {2023-03-14},
	journal = {Google Developers},
	file = {Snapshot:D\:\\Zotero\\storage\\RWW6WQPQ\\reducers_intro.html:text/html},
}

@article{saad_el_imanni_multispectral_2023,
	title = {Multispectral {UAV} data for detection of weeds in a citrus farm using machine learning and {Google} {Earth} {Engine}: {Case} study of {Morocco}},
	volume = {30},
	issn = {2352-9385},
	shorttitle = {Multispectral {UAV} data for detection of weeds in a citrus farm using machine learning and {Google} {Earth} {Engine}},
	url = {https://www.sciencedirect.com/science/article/pii/S235293852300023X},
	doi = {10.1016/j.rsase.2023.100941},
	abstract = {Accurate and timely weed mapping between and within trees is considered one of the major challenges in the site-specific weed management systems. This research presents the first cloud computing approach based on the incorporation of multispectral Unmanned Aerial Vehicle (UAV) imagery in the Google Earth Engine (GEE) programming environment with the aim of improving the mapping of weed patches between and within trees in a citrus farm. For this purpose, the UAV multispectral bands (red, green, blue, near infrared, and red edge), as well as the estimated vegetation height from the UAV Digital Elevation Model (DEM) were analysed in terms of tree and weed discrimination and used as input into Random Forest (RF) and k-nearest neighbors (KNN) machine learning algorithms. From the DEM, the Digital Terrain Model (DTM) was estimated using the Inverse Distance Weighted Interpolation (IDW) of the elevation values of dense points on the soil. The plant height was derived by subtracting the DTM from the DEM, resulting in the Canopy Height Model (CHM). The experimental results show that: (i) the combination of spectral bands and the CHM can classify both trees and weeds with an overall accuracy reached 96.87\%; (ii) the RF classifier was more robust compared to KNN in the classification performance; (iii) when compared to the use of UAV spectral bands, the addition of the CHM can improve the accuracy of crop classification by 13.36\% (KNN) and 1.79\% (RF). Furthermore, the integration of UAV imagery in the GEE was highly efficient in terms of automation of the UAV imagery processing.},
	language = {en},
	urldate = {2023-03-14},
	journal = {Remote Sensing Applications: Society and Environment},
	author = {Saad El Imanni, Hajar and El Harti, Abderrazak and Bachaoui, El Mostafa and Mouncif, Hicham and Eddassouqui, Fatine and Hasnai, Mohamed Achraf and Zinelabidine, Moulay Ismail},
	month = apr,
	year = {2023},
	keywords = {Canopy height model (CHM), Google earth engine (GEE), Machine learning, Multispectral UAV imagery, Weed mapping},
	pages = {100941},
	file = {ScienceDirect Snapshot:D\:\\Zotero\\storage\\8PIR2GSZ\\S235293852300023X.html:text/html},
}

