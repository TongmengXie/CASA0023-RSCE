[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Diary - CASA0023",
    "section": "",
    "text": "Introduction\nWelcome to my learning diary page of Remote Sensing Cities and Environment (CASA0023)! This diary is made for the content taught at 2022-2023.\nI’m a current Master of Science student at Bartlett Centre for Advanced Spatial Analysis"
  },
  {
    "objectID": "Week1.html",
    "href": "Week1.html",
    "title": "1  Week 1",
    "section": "",
    "text": "This section summarises the lecture content and a graph of feature space derived from practical in SNAP operations.\nPassive data: Energy usually in eletcromagnetic form e.g., human eyes\nActive data: Energy in addition in illumination . e.g., radar.\nHow EM waves interact with Earth’s surface and atmosphere: Reflection, scattering, absorption\nsingle\ndual\nquad\n\n\nRaster: file\ntypes: BIL, BSQM BIP, GeoTIFF\n\n\n\nSpatial: ranging from 10 cm to several kilos\nSpectral: How many different spectral bands are there? (Every feature on earth has a unique spectral signature)\n(Atmospheric windows: )\n(Vegetation: red edge -- infra bands. APP: look at the infra band s of city to identify who has access to vegetation)\nRadiometric resolution: resolution of cell’s value\nTemporal resolution: ussualy inversly relate3 to pixel size (spatial)\nMODIS\n\n\n\n\n\n\nFigure 1.1: Spectral Feature Space, Vegetation On Bands B04 and B08"
  },
  {
    "objectID": "Week1.html#application",
    "href": "Week1.html#application",
    "title": "1  Week 1",
    "section": "1.2 Application:",
    "text": "1.2 Application:\n“Spectral Feature Space, Vegetation On Bands B04 and B08”\nOne of the applications really attracted me was the spatial signature of vegetation on the terra, as we could assign features to each end of the spatial signature area see Figure 1.1, such as bare land on the right end of the triangle-like area where red light captured are dense while near-infrared level is low. Heavy vegetation are witnessed at the upper end of the triangle-like area where red light low and near-infrared is high, indicating heavy biomass. As for the left-down corner where both red and near-infrared are low, we can identify wet lands. This is integrated in the NDVI (Normalized Difference Vegetation Index) to estimate vegetation cover.\nSpatial signatures can also be used to monitor the health of vegetation by identifying patterns of quavariation in spectral reflectance that are indicative of stress or disease. For example, vegetation that is stressed or diseased may have a different spectral reflectance signature than healthy vegetation, which can be identified using spatial signatures.\nIn addition, spatial signatures can be used to monitor the growth and distribution of vegetation over time by comparing satellite imagery from different dates. This can be useful for understanding the impacts of land use changes, climate change, and other factors on vegetation.\nOverall, spatial signatures are a powerful tool for vegetation monitoring, as they can be used to identify and classify different types of vegetation, monitor vegetation health, and track vegetation changes over time."
  },
  {
    "objectID": "Week1.html#reflection",
    "href": "Week1.html#reflection",
    "title": "1  Week 1",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\njust state what interest you and why, as well as the application. Application: Context matters. Why useful? What had it assisted achieving. Mind map of concepts, to show understanding of data and workflow\nOne of the challenges I encountered is to navigate the complexities of the interface of SNAP and QGIS. It becomes clear to me that yes implementing several functions in code can be challenging, but a software with collective functions as a whole can be mindblowing even when with decent GUIs. Specifically, finding which function falling under which menu consumes a lot of time, and figuring out filling parameters to carry the analysis also took some efforts of iterative validation.\nWhen doing the operation in R on a script level, it becomes confusing where I put the data"
  },
  {
    "objectID": "Week3.html",
    "href": "Week3.html",
    "title": "3  Week 3 - Remote sensing data",
    "section": "",
    "text": "In this week’s learning diary, we try to handle"
  },
  {
    "objectID": "Week3.html#summary",
    "href": "Week3.html#summary",
    "title": "3  Week 3 - Remote sensing data",
    "section": "3.1 Summary:",
    "text": "3.1 Summary:\n\n3.1.1 Different Sensors\nAcross track scanners: Mirror reflects light onto 1 detector. For example, Landsat dataset are captured by this sort\nAlong track scanners: Basically several detectors pushed along. E.g., Quickbird, SPOT\n\n\n3.1.2 Geometric Correction\nRS data could include image distortions introduced by: View angle, topography, wind and rotation of the earth\nWe identify Ground Control Points (GCP) in distorted data to match them with local map, correct image, or GPS data from handheld device, but these reference images could also contain distortions and imprecisions.\nRMSE is adopted here to measure fitness between images. Use GCPs to minimise RMSE.\nDoing geometric correction can shift the original image, so we want to re-sample the final raster by using Nearest Neighbour, Linear, Cubic, Cubic spline re-samplers\n\n\n3.1.3 Atmosphric Correction\nAccording to Jensen (1986), two factors contribute to environmental attenuation: Atmospheric scattering, topographic attenuation.\nThere are unnecessary and necessary atmospheric corrections:\nnecessary ones are:\n\nBiophysical parameters needed (e.g. temperature, leaf area index, NDVI)\nE.g. .. .NDVI is used in the Africa Famine Early Warning System and Livestock Early Warning System\nUsing spectral signatures through time and space\n\nAbsorption and scattering can create the haze, i.e. reduces contrast of image.\nScattering can create the “adjacency effect”, radiance from pixels nearby mixed into pixel of interest.\n\n\n3.1.4 Orthorectification Correction\nThis is a subset of georectification, i.e. giving coords to an image. Particularly Orthorectification means removing distortion so pixels can appear being viewed at nadir (straight down). This requires the support of an Elevation Model to calculate the nadir view for each pixel on a sensor geometry.\nTo do this: cosine correction, Minnaert correction, Statistical Empirical correction, C Correction (advancing the Cosine). Need radiance (DN to TOA) from sloped terrain, Sun’s zenith angle, Sun’s incidence angle - cosine of the angle between the solar zenith and the normal line of the slope. Latter two found in angle coefficient files (e.g. Landsat data ANG.txt).\n\n\n3.1.5 Rdiometric Correction\nCorrections to raw satellite imagery can be performed using a method called Dark Object Subtraction (DOS). The logic is that the darkest pixel in the image should be 0 and any value it has is due to the atmosphere. To remove the atmospheric effect, the value from the darkest pixel is subtracted from the rest of the pixels in the image. The calculation involves converting the Digital Number (DN) to radiance, computing the haze value for each band (but not beyond NIR), and subtracting the 1% reflectance value from the radiance. The calculation requires values such as mean exoatmospheric irradiance, solar azimuth, Earth-sun distance, and others, which can be found in sources such as Landsat user manuals.\n\n\n3.1.6 Joining data sets\nAlso known as Mosaicking: We feather two images, creating a seamless mosaic, where the diving lien is called seamline.\n\n\n3.1.7 Image Enhancements\nImage stretch, Band ratioing, Normalised Burn Ratio, Edge enhancement, Filtering, PCA, Image fusion (see application) etc."
  },
  {
    "objectID": "Week3.html#application---discussing-image-fusion-in-one-literature",
    "href": "Week3.html#application---discussing-image-fusion-in-one-literature",
    "title": "3  Week 3 - Remote sensing data",
    "section": "3.2 Application - Discussing image fusion in one literature",
    "text": "3.2 Application - Discussing image fusion in one literature\n\nFrom literature we delve in the nuances of levels on which we perform image fusion to acquire better results. The integration methods vary as the levels vary (Schulte to Bühne and Pettorelli 2018).\nSatellite remote sensing (SRS) can be derived from Multispectral sensors and radar sensors. \n Multispectral sensors are passive, merely receiving electromagnetic waves reflected from surface, usually used to reflect chemical properties (such as nitrogen or carbon content and moisture). Usually produces data with comparatively low spatial resolution\n Radar ones emit electromagnetic radiation and measure the returning signal, responding to the three-dimensional structure of objects, being sensitive to their orientation, volume and surface roughness. Usually produces data with comparatively high spatial resolution\n\n3.2.1 Image fusion:\n1. decision-level (SRS integration), where separate predictors are used to estimate a parameter of interest. \n2. object-level (feature-level). unit: multi-pixel objects. (1) using radar and multispectral imagery is input into an object-based image segmentation algorithm, or (2) segmenting each type of imagery separately before combining them. multi-pixel objects\n3. pixel-level (Observation-level), where pixel values are combined to derive a fused image with new pixel values, either in the spatial or the temporal domain.\n(2. and 3. derive entirely new predictors.)\n\n\n\nFigure 3.1: Credit: Schulte to Bühne and Pettorelli (2018)\n\n\nSchematic overview of multispectral-radar SRS data fusion techniques. The parameter of interest can be a categorical variable, like land cover, or a continuous variable, like species richness. In pixel-level fusion, the original pixel values of radar and multispectral imagery are combined to yield new, derived pixel values. Object-based fusion refers to (1) using radar and multispectral imagery is input into an object-based image segmentation algorithm, or (2) segmenting each type of imagery separately before combining them. Finally, decision-level fusion corresponds to the process of quantitatively combining multispectral and radar imagery to derive the parameter of interest (by e.g. combining them in a regression model, or classification algorithm)\n\n\n3.2.2 Implementation Approaches\n\n\n\nFigure 3.2: Credit: Schulte to Bühne and Pettorelli (2018)\n\n\npixel-level\n\nComponent substitution techniques: such as principal component analysis (PCA), Intensity-hue-saturation (IHS). \n\nPCA is the only pixel-level image fusion technique that cannot be applied to imagery with different spatial resolutions, and the only that allows unlimited image numbers.\n\nIHS fusion. Three images with lower spatial resolution (typically multispectral data) are integrated with a single image with high spatial resolution (typically radar) to retain the radiometry but increase the spatial resolution of the former. Facilitate visual interpretation by combining resulting images into a single RGB image.\n\nMulti-resolution analysis, such as **Wavelet transformation. Decompose multispectral and radar imagery into their respective low- and high-frequency components\n\nArithmetic fusion techniques: such as the Brovey transform algorithm. Unlikely to be appropriate for multispectral-radar SRS image fusion.\n\nObject-level: Based on brightness and intensity values of each pixel, as well as its spatial context, objects such as lines, shapes or textures are extracted.\n1. image segmentation: Demands that multispectral and radar SRS images are with the same spatial resolution\n2. *extracting objects separately and combining in a feature map*\nObject-based fusion reduces all multispectral and radar information into a single layer of discrete objects, which are often relatively easy to relate to ecological features.\nDecision-level fusion: Quantitative decision-making frameworks—such as a regression, a quantitative model or a classification algorithm."
  },
  {
    "objectID": "Week3.html#reflection",
    "href": "Week3.html#reflection",
    "title": "3  Week 3 - Remote sensing data",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nData correction, Data fusion and Image enhancement SRS data fusion can increase the quality of SRS (Satellite Remote sensing)-derived parameters for application in terrain detection, urban analysis, ecology and conservation (Schulte to Bühne and Pettorelli 2018). It is thus important to explore how best to capitalise on recent technological developments and changes in SRS data availability. It is exctiing to apply solid machine learning methods to this area and it is marvelous to see the progress reflected by the increasing number of software supporting this application. The improvement of image quality enables new research designs in ecology and conservation areas and reignite previously greyed-out options.\nThe application of data correction, data fusion, and image enhancement techniques to SRS data can greatly improve the accuracy and reliability of SRS-derived parameters, which can then be used in various fields, including terrain detection, urban analysis, ecology, and conservation. With the rapid advancements in technology and the increasing availability of SRS data, there is a growing opportunity to leverage the latest machine learning techniques in this area. The development of new software tools to support these applications is a testament to the progress being made in this field. By enhancing the quality of the SRS data, researchers are able to design more robust and informative studies, unlocking new insights and avenues for exploration in ecology and conservation. This, in turn, has the potential to lead to breakthroughs and innovations in these fields, making a significant impact on the world around us.\n\n\n\n\nJensen, J. Robert. 1986. “Introductory Digital Image Processing: A Remote Sensing Perspective.” In.\n\n\nSchulte to Bühne, Henrike, and Nathalie Pettorelli. 2018. “Better Together: Integrating and Fusing Multispectral and Radar Satellite Imagery to Inform Biodiversity Monitoring, Ecological Research and Conservation Science.” Methods in Ecology and Evolution 9 (4): 849–65. https://doi.org/10.1111/2041-210X.12942."
  },
  {
    "objectID": "Week5.html",
    "href": "Week5.html",
    "title": "5  Week 5 - An introduction to Google Earth Engine",
    "section": "",
    "text": "This week introduces Google Earth Engine (GEE), a geospatial processing service that allows for planetary scale analysis of massive datasets in seconds.\nBasics:\nObjects and methods in GEE are introduced:\nAlso, the types of analyses that can be performed in GEE are briefly covered."
  },
  {
    "objectID": "Week5.html#summary",
    "href": "Week5.html#summary",
    "title": "5  Week 5 - An introduction to Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\n\n5.1.1 GEE Basics\nJavaScript, where objects are dictionaries:\n\nWe have ee (EarthEngine), a powerful package. Anything starting with ee (proxy objects) are stored on the server.\nProblems:\n\nWe don’t iterate the data on the server; instead, we map (using a mapping function) them into objects (variables) so we only load them once.\nThere are also some sort of server-wide functions.\nAvoid using loops in GEE on the server-side, as mapping can automatically detect the number of loops needed.\n\n\nScale:\n\nPixel resolution, set by the output.\nGEE does resampling, aggregating your input to a 256*256, mainly down-sampling.\n\nTable 1: Terms and Jargon Related to Google Earth Engine\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGoogle Earth Engine\nA geospatial processing service that allows geospatial analysis at scale.\n\n\nImage\nRefers to raster data in GEE and has bands.\n\n\nFeature\nRefers to vector data in GEE and has geometry and attributes.\n\n\nImageCollection\nA stack of images in GEE.\n\n\nFeatureCollection\nA stack of features (lots of polygons) in GEE.\n\n\nProxy objects\nGEE objects that are stored on the server and have no data in the script.\n\n\n\nTable 2: Differences between Client and Server Side in Google Earth Engine\n\n\n\n\n\n\n\nAspect\nDefinition\n\n\n\n\nClient Side\nRefers to the browser side of GEE.\n\n\nServer Side\nRefers to the side of GEE where data is stored.\n\n\nEarth Engine Objects\nObjects in GEE starting with “ee”.\n\n\nLooping\nLooping is not recommended for objects on the server side.\n\n\nMapping\nInstead of loops, mapping is used in GEE to apply a function to everything on the server.\n\n\nScale\nScale refers to pixel resolution in GEE. The scale is set by the output, not the input, and Earth Engine selects the pyramid with the closest scale to analysis.\n\n\n\n\n\n5.1.2 GEE Objects\nObjects:\n\nImages (Rasters), geometry, ImageCol, features, featureCol, joins, arrays, chart.\n\nTable 3: Geometry Types and Features\n\n\n\n\n\n\n\nType of Geometry\nDescription\n\n\n\n\nPoint\nA single location represented by its longitude and latitude\n\n\nLine\nA series of connected points representing a linear feature\n\n\nPolygon\nA closed shape with three or more sides, represented by a series of connected lines forming a closed loop\n\n\nMultiPolygon\nA collection of polygons, where each polygon is represented as a list of coordinate tuples defining its vertices\n\n\nMultiGeometry\nA collection of different types of geometries\n\n\n\n\n\n5.1.3 GEE Processes and Applications/Outputs\nGEE applications:\n\nReducing types.\nDifferent to filterBounds() that filters the area of interest, to do zonal statistics, we have reduceRegion(), where regions are subcategories of the area of interest.\nAlso, we have reduceNeighborhood(), which is a bit like a kind of image enhancement.\n\nLinear Regressions:\n\nIn a scenario of visualising precipitation, we can do a multivariate multiple linear regression where both independent variables (time) and dependent (precip, temp) variables are multiple.\nSomething about constant bound.\n\nJoins:\n\nIn GEE, everything, e.g. within a buffer, intersect, etc. needs the mediation of Join (apply()).\nTo perform joins, we need to put data into Filter().\n\nClassifiers:\n\nPer-pixel\nsub-pixel\n\nTable 4: GEE Processes and Applications/Outputs\n\n\n\n\n\n\n\nProcess\nDescription\n\n\n\n\nGeometry operations\nSpatial operations such as union, intersection, buffer, and distance analysis\n\n\nJoins\nCombining two feature collections based on a shared attribute value\n\n\nZonal statistics\nComputing statistics for a region or set of regions such as mean, median, and mode of pixel values within a feature or a collection of features\n\n\nFiltering\nFiltering of images or specific values based on criteria such as date range, location, and attribute value\n\n\nMachine learning\nUsing statistical and machine learning algorithms for classification, clustering, and prediction tasks\n\n\nDeep learning\nA subset of machine, using Deep Neural Networks\n\n\n\n\n\n5.1.4 Limitations\nNo support for phase data, needs SNAP."
  },
  {
    "objectID": "Week5.html#application",
    "href": "Week5.html#application",
    "title": "5  Week 5 - An introduction to Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application"
  },
  {
    "objectID": "Week5.html#reflection",
    "href": "Week5.html#reflection",
    "title": "5  Week 5 - An introduction to Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nGEE-using skills can be a valuable asset for a spatial data scientist, as it allows for complex spatial analysis at scale. Traditional GIS software is eclipsed when it comes to both efficiency and scale.\nGEE’s unique and efficient way of conducting analysis flows is interesting, such as the introduction of concepts like client vs server-side operations and data reduction techniques. These was required by GEE’s feature of carrying out analyses on massive datasets (Gorelick et al. 2017). For those interested in BigData technology, the strategies (server/client split, no looping on server, etc.) applied by Google here is a very resourceful one and worth learning. The user end also has to learn to adopt good practices for reducing data range, which has been simplified to a series of reduction and filtering functions, e.g. ImageCollection.filterDate(), image.reduceNeighborhood()(Google 2023b).\nGEE’s combination with machine learning is also promising in regard of automating complex analysis tasks, as Machine Learning APIs offered by GEE support Supervised and Unsupervised Classification, and Regression (Google 2023a). According to Saad El Imanni et al. (2023), as a subtask of intelligent agriculture, weeds detection task sees an impressive performance (overall accuracy reached 96.87%) when GEE and Machine learning are combined.\n\n\n\n\nGoogle. 2023a. “Machine Learning in Earth Engine  Google Earth Engine.” Google Developers. https://developers.google.com/earth-engine/guides/machine-learning.\n\n\n———. 2023b. “Reducer Overview  Google Earth Engine.” Google Developers. https://developers.google.com/earth-engine/guides/reducers_intro.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-Scale Geospatial Analysis for Everyone.” Remote Sensing of Environment, Big Remotely Sensed Data: Tools, applications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nSaad El Imanni, Hajar, Abderrazak El Harti, El Mostafa Bachaoui, Hicham Mouncif, Fatine Eddassouqui, Mohamed Achraf Hasnai, and Moulay Ismail Zinelabidine. 2023. “Multispectral UAV Data for Detection of Weeds in a Citrus Farm Using Machine Learning and Google Earth Engine: Case Study of Morocco.” Remote Sensing Applications: Society and Environment 30 (April): 100941. https://doi.org/10.1016/j.rsase.2023.100941."
  },
  {
    "objectID": "Week6.html",
    "href": "Week6.html",
    "title": "6  Wk6 Classification",
    "section": "",
    "text": "Information\nSummary\n\n\n\n\nPurpose of classification\nTo subset data into classes or values, such as landcover or estimating values like GCSE scores or pollution.\n\n\nDifferent classification methods\nEssentially slice the data in different ways.\n\n\nComplexity of classification methods\nThey can often be made to appear more complicated than they are.\n\n\nControlling classifiers\nCan be done using hyperparameters.\n\n\nDesired outcome of classifiers\nCan range from a single tree to a decision hyperplane boundary in multiple dimensions.\n\n\n\n\n\nTable 1: Supervised Classification Methods\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nMaximum Likelihood\nA statistical method used to estimate the parameters of a probability distribution based on observed data.\n\n\nSupport Vector Machines (SVM)\nA supervised learning algorithm that finds the best hyperplane to separate data into different classes.\n\n\n\nTable 2: Unsupervised Classification Methods\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nDensity Slicing\nDivides the range of pixel values into equal intervals and assigns each interval a unique class value.\n\n\nParallelpiped\nUses a set of user-defined ranges for each band to define class boundaries in multi-dimensional space.\n\n\nMinimum Distance to Mean\nAssigns each pixel to the class with the closest mean value in multi-dimensional space.\n\n\nNearest Neighbor\nAssigns each pixel to the class of its nearest neighbor in multi-dimensional space.\n\n\n\nTable 3: Other Machine Learning Methods\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nArtificial Neural Networks (ANN)\nA set of algorithms inspired by the structure and function of biological neural networks, used for pattern recognition and prediction tasks.\n\n\n\n\n\n\nTable 1: Supervised vs. Unsupervised Classification\n\n\n\n\n\n\n\n\nClassification Type\nDefinition\nMethod\n\n\n\n\nSupervised\nClassifier learns patterns in the data and uses that to place labels onto new data. Pattern vector is used to classify the image. Usually, pixels are treated in isolation but as we have seen - contextual (neighboring pixels), objects (polygons), texture.\nPattern recognition or machine learning\n\n\nUnsupervised\nIdentifies land cover classes that aren’t known a priori (before) and tells the computer to cluster based on info it has (e.g. bands) and label the clusters.\nDensity slicing, parallelpiped, minimum distance to mean, nearest neighbor, neural networks, machine learning / expert systems*\n\n\n\n\n\n\n\nBias refers to the difference between the predicted value and the true value. When a model has high bias, it is too simple and may underfit the data. On the other hand, when a model has low bias, it may overfit the data.\nVariance, on the other hand, refers to the variability of a model for a given point. When a model has high variance, it is too complex and may overfit the data. This means that it will perform well on the training data but poorly on new data.\n\n\n\n\nFigure 6.1: Credit: CASA0006\n\n\nIn general, overfitting occurs when there is a trade-off between bias and variance. A model with high bias and low variance will underfit the data, while a model with low bias and high variance will overfit the data. The goal is to find a balance between bias and variance that results in good performance on both training and test data.\n\n\n\nFigure 6.2: Credit: CASA0006\n\n\n\n\n\nEarth Observation (EO) data classification is continually evolving, with new technologies and techniques leading to several anticipated future developments:\n\nMulti-source data fusion:\n\nIntegrating data from multiple sources like satellite imagery, LiDAR, and ground-based sensors will become more prevalent. This fusion enhances classification accuracy and offers comprehensive Earth’s surface information, improving decision-making and monitoring. For example, the European Union’s Copernicus Programme could use this in providing free data from various satellite missions and sensors for environmental monitoring, disaster management, and urban planning.\n\nMulti-temporal analysis:\n\nSophisticated multi-temporal analysis techniques will be increasingly used to monitor changes in land cover, vegetation, and other features over time. This enables accurate and efficient change detection and monitoring of phenomena like urbanization, deforestation, and climate change. This aligns with the REDD+ initiative under the United Nations Framework Convention on Climate Change (UNFCCC), which uses multi-temporal analysis to monitor forest cover changes and evaluate policy effectiveness in reducing greenhouse gas emissions from deforestation and forest degradation.\n\nCloud-based processing:\n\nThe growth of cloud-based platforms, such as Google Earth Engine, allows for more efficient and scalable EO data processing workflows. This accessibility enables researchers and organizations to innovate in classification techniques and applications. The National Oceanic and Atmospheric Administration’s (NOAA) Big Data Project aims to make vast amounts of environmental data accessible and usable in the cloud, fostering innovation in developing new applications and services.\n\n\nThese advancements will provide comprehensive and timely information about Earth’s surface, informing policies and strategies in areas such as environmental management, disaster response, and urban planning."
  },
  {
    "objectID": "Week7.html",
    "href": "Week7.html",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "",
    "text": "This week’s learning diary continues that from Week 6 in addressing the big problem in Remote Sensingm, i.e. classification within Earth Observation data. Also, accuracy metrics are discussed."
  },
  {
    "objectID": "Week7.html#summary",
    "href": "Week7.html#summary",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nThe summary of lecture content as well as practical outcomes. See ?fig-mindmap for an overview. If certain words are intelligible due to resolution issues, hopefully you can right click and “open in new page” to get a better view since this is a .SVG file. \n\n7.1.1 Data\n\nSurface Reflectance (SR)\nTop of Atmosphere (TOA)\n\nA mixed way of doing urban recognition\n\n\n7.1.2 OBIA (object-based image analysis)\nInstead of a per-pixel approach, we adopt an object-based image analysis (OBIA), where you have to manually create objects.\nSLIC (Simple Linear Iterative Clustering) (2012): No ground truth\nDescent, similarity (Homogeneity)\n\n\n7.1.3 Sub-pixel analysis\nSMA (Spectral maximum analysis), SPC, LSU\nThrough a series of manipulation of material, we acquire a list describing the broken-down land cover of that pixel\n\n7.1.3.1 Pixel purity\nEndmember: an important concept in spectral mixture analysis\nIn remote sensing, an end member refers to a pure or nearly pure material or component that is present within a mixed pixel.\nIn spectral mixture analysis, the spectral signature of a mixed pixel is modelled as a linear combination of the spectral signatures of the constituent endmembers, with each end member being assigned a proportion or fraction that represents its contribution to the overall reflectance or radiance of the mixed pixel.\n\n\n\n7.1.4 Accuracy assessment\n\nProducer accuracy: Recall\nUser’s accuracy: Precision\nOverall accuracy: not equivalent to F1\nKappa coefficient: [0, 1], measures how good the classification is compared to random distribution e.g. Poisson. Different interpretations of this metric, problematic\n\nMake a tradeoff between Producer accuracy and User accuracy, by shifting the decision boundary.\n\nF1: issue: TN not considered; Recall and precision are not equally important yet equally weighted in F1\nReceiver Operating Characteristic Curve: True positive rate and false positive rate are all good. We want to maximise the area under the curve in a True positive rate vs false positive rate plot.\n\n\n\n7.1.5 Workflow\n\n(Potentially use unsupervised classification to understand your data\nClass definition (Potentially use unsupervised classification)\nPreprocessing\nTraining\nPixel Assignment\nAccuracy assessment\n\nPseudo-invariant features to be trained on to make your model robust to time-space changes\nPseudo-invariant features are often used as reference targets or calibration sites in remote sensing to account for changes in sensor or atmospheric conditions and to reduce the effects of noise and calibration drift on image data. These features have relatively constant spectral properties over time and space, and can therefore serve as a stable reference for monitoring changes in other features or materials within an image or scene.\nA flow chart can be seen in Figure 7.1 :\n\n\n\nFigure 7.1: Classification Workflow, courtesy: myself\n\n\n\n\n7.1.6 A “Sneak preview” (Analogous to Data Leakage in ML)\nWaldo Tobler’s first law of geography indicates that if training and testing are spatially close, the training can cause the problem of a sneak preview.\n\n7.1.6.1 Spatial Cross Validation\nSimilar to cross-validation but adds clustering to folds.\nIn spatial cross-validation, the data are split into spatially contiguous blocks or subsets, rather than randomly shuffled subsets as in traditional cross-validation. This is done to ensure that the model is tested on data that are spatially distinct from the data used to train the model and to account for spatial autocorrelation and other spatial dependencies in the data.\n\n\n\n7.1.7 Approaches to deal with Spatial Autocorrelation\nObject-based image classification\nMoran’s I (Spatial Cross Validation)"
  },
  {
    "objectID": "Week7.html#application---to-be-completed",
    "href": "Week7.html#application---to-be-completed",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.2 Application - to be completed",
    "text": "7.2 Application - to be completed\nIn remote sensing, it is often challenging to accurately classify mixed pixels, which contain a combination of different materials or components. Endmembers refer to pure or nearly pure materials that are present within a mixed pixel. By modelling the spectral signature of a mixed pixel as a linear combination of the spectral signatures of the constituent endmembers, we can determine the contribution of each endmember to the overall reflectance or radiance of the mixed pixel.\nThis approach can be very useful in urban recognition, where it is essential to accurately classify the different land covers present within a pixel. Furthermore, it can also help us to understand the composition of the land cover in a given area, which can have important implications for environmental monitoring and management. This approach has been applied in various studies to estimate urban land cover, such as the work by Zhang et al. (2018) that utilised endmember extraction to detect urban impervious surfaces."
  },
  {
    "objectID": "Week7.html#reflection",
    "href": "Week7.html#reflection",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nThe workflow of Classification of Surface Reflectance and Top of Atmosphere data intrigues me, as it differs from, yet shares certain features with traditional computer vision tasks like image classification and object detection (in regards of treating pixels as objects/units). Alternatively, Surface Reflectance Classification can be treated as a downstream task for both aforementioned ML tasks, due to its uniqueness in dealing with high-precision satellite data and unseparability (worth debating) from EO processing workflow (calibration etc.). Also, uncertainty of classification genres derived from unsupervised labelling can also be an issue.\nOptionally, in supplementation to manual labelling, automated labelling workflow (e.g. roboflow) can be introduced to curtail repeatitive works in image labelling(Nair, Paul, and Jacob 2018). However, manual labelling is not replaceable at current time (Robison 2018) and the pinning down of ground truth seems to always need human intervention in addition to machine automation.\nThe introduction of production accuracy and user accuracy is also interesting, as these terminologies are designed presupposing a customer/producer split, dwarfing precision-recall in readability. The treadeoff to be made between the two is crucial, and this is problematically handled by introducing F1 score with two competitive components, and improved by introducing Receiver Operating Characteristic Curve (ROCC) with two “good” indicators, true positive rate and false positive rate."
  },
  {
    "objectID": "Week8.html",
    "href": "Week8.html",
    "title": "8  Week8 - Temperature and Policy",
    "section": "",
    "text": "The “policy” section occupies two weeks, mainly trying to introduce how to fit EO data workflow into current policies. To do this, you have to identify the gaps, e.g., that between the overarching global policies, metropolitan plans and local plans. Or, the gap within policies like missing locations in the Singapore one."
  },
  {
    "objectID": "Week8.html#summary",
    "href": "Week8.html#summary",
    "title": "8  Week8 - Temperature and Policy",
    "section": "8.1 Summary",
    "text": "8.1 Summary\n\n8.1.1 Urban Heating Islands (UHI) problem and plans\n\n8.1.1.1 Causes\nUrban areas have comparatively higher atmospheric and surface temperatures than surrounding rural areas, mainly due to\n\nMore dark surfaces that retain heat\nLess vegetation that cools the environment\n\nAlso, there are other contributors to the heat:\n\n\n\nContributor\nCorrelation with UHI phenomenon\n\n\n\n\nSky View Factor (SVF)\nPositive\n\n\nAir speed\nNegative\n\n\nHeavy cloud cover\nPositive\n\n\nCyclic solar radiation\nPositive\n\n\nBuilding material type\nVaries\n\n\nAnthropogenic energy\nPositive\n\n\n\n\n\n8.1.1.2 Cost\nThe cost of the Urban Heat Island can be divided into social, environmental, and economic costs.\n\n\n\n\n\n\n\nType of Cost\nExamples\n\n\n\n\nSocial Costs\nPopulation-adjusted excess mortality rates, heat-related deaths\n\n\nEnvironmental Costs\nIncrease in fossil fuel usage\n\n\nEconomic Costs\nLoss of Gross Domestic Product (GDP)\n\n\n\nFor instance, under a low greenhouse gas scenario, the percent GDP lost from UHI is estimated to be 0.71% in 2050.\n\n\n8.1.1.3 Plans\n\n8.1.1.3.1 Global\n\n\n8.1.1.3.2 Local\n\n\n\n\n\n\n\n\nCity\nInitiatives\nTechnology\n\n\n\n\nSingapore\nGreen buildings\nUrban greenery\n\n\nMedellin\nGreen Corridors\nUrban greenery\n\n\nSydney\nTurn Down the Heat Strategy and Action Plan\nReflective roofs/pavements/sidewalks, Cool roads trial in Western Sydney"
  },
  {
    "objectID": "Week8.html#application",
    "href": "Week8.html#application",
    "title": "8  Week8 - Temperature and Policy",
    "section": "8.2 Application",
    "text": "8.2 Application\nMacLachlan et al. (2021) The document outlines a sub-city urban planning modeling approach using open-source tools to measure and monitor localised urban heat island (UHI) mitigation targets. The methodology involves comparing temperature dynamics of low- and high-density census areas using Earth observation data and determining optimal placement of greening elements in proposed plans using a data-driven model. The document concludes that this approach can be universally integrated into urban planning regulation frameworks to mitigate the localized UHI effect and ensure long-term city sustainability. Also it discusses the impact of low population density on housing in Perth, Australia, and the resulting need for strategic land zonation and sustainability targets. #### Why Data-driven approach\n\n8.2.1 Policy limitations\n\nlacking specificities for combating adverse temperature effects at the local level (sub-city), therefore not planning practicality\nno consistency in planning implementation methodologies or steps for assessing progress toward UHI reduction targets\nlackage of empirical evidence for optimizing UHI mitigation strategies\n\n\n\n8.2.2 Data to drive the new approach\n\nEarth Observation (EO) data can be processed to identify (un)sustainable urban development through aerial assessments of land cover change\ntemperature\nelevation\n\n\n\n8.2.3 Advantages for the Data-driven approach\n\nEO data can produce consistent information necessary for restricting unsustainable development\nmonitor UHI effects based on associated land-temperature dynamics\nAssessment at finer spatial scales (e.g., block subdivisions)"
  },
  {
    "objectID": "Week8.html#methodology",
    "href": "Week8.html#methodology",
    "title": "8  Week8 - Temperature and Policy",
    "section": "8.3 Methodology",
    "text": "8.3 Methodology\n\n\n8.3.1 Temperature Modeling\n\n\nModeled temperature every 3 hours using SOLWEIG model between 2008 and 2010\n\n\n\nInputs generated from meteorological data, land cover, building DSM, ground DTM, and vegetation canopy REM in QGIS\n\n\n\nSVF computed from vegetation canopy REM, building DSM, and ground DTM using UMEP plugin\n\n\n\nBuilding wall heights and aspect generated from DSM and DTM using UMEP plugin\n\n\n\n\n8.3.2 Data-Driven Tree Placement\n\n\nSite selected for modeling temperature in the City of Fremantle\n\n\n\nThree scenarios processed: current urban footprint, proposed changes, and proposed redevelopment with no trees\n\n\n\nHighest temperatures identified and used to redesign tree placement\n\n\n\n15 trees distributed according to original design aspects\n\n\n\nUpdated vegetation canopy REM reflected new tree locations\n\n\n\nAnalysis re-run to compare temperature across redevelopment site\n\n\n\nModeled all scenarios accounting for influence of neighboring landscape features"
  },
  {
    "objectID": "Week8.html#result",
    "href": "Week8.html#result",
    "title": "8  Week8 - Temperature and Policy",
    "section": "8.4 Result",
    "text": "8.4 Result\nTable 1: Results of assessments of urban design factors on UHI effect in Perth.\n\n\n\nFactors assessed\nEffect on UHI effect\n\n\n\n\nVegetation cover\nNegative correlation with UHI effect\n\n\nCanopy cover\nNegative correlation with UHI effect\n\n\nBuilding density\nPositive correlation with UHI effect\n\n\nBuilding height\nPositive correlation with UHI effect\n\n\nAlbedo\nNegative correlation with UHI effect\n\n\nLand use\nNegative correlation with UHI effect\n\n\nUrban sprawl\nPositive correlation with UHI effect\n\n\n\nTable 2: Total population and population density per 0.1 km2 between 2011 and 2016 for Subiaco and Currambine SA1s as defined by the ABS.\n\n\n\n\n\n\n\n\n\n\nSA1\nTotal Population (2011)\nTotal Population (2016)\nPopulation Density per 0.1 km2 (2011)\nPopulation Density per 0.1 km2 (2016)\n\n\n\n\nSubiaco\nN/A\nN/A\n325\n712\n\n\nCurrambine\nN/A\nN/A\n310\n324"
  },
  {
    "objectID": "Week8.html#reflection",
    "href": "Week8.html#reflection",
    "title": "8  Week8 - Temperature and Policy",
    "section": "8.5 Reflection",
    "text": "8.5 Reflection\n\nCase study: Superblocks, Barcelona\n\nBasically about pedestrian economy. Though there have been many retail modes like KFC and other American fast food, the experience from Europe tells that economy vitality can have a boost with pedestrian-dominated areas. See Barcelona\n\n\n\n\nMacLachlan, Andrew, Eloise Biggs, Gareth Roberts, and Bryan Boruff. 2021. “Sustainable City Planning: A Data-Driven Approach for Mitigating Urban Heat.” Frontiers in Built Environment 6. https://www.frontiersin.org/articles/10.3389/fbuil.2020.519599."
  },
  {
    "objectID": "Week9.html",
    "href": "Week9.html",
    "title": "9  Week 9 - Synthetic Aperture Radar (SAR) data",
    "section": "",
    "text": "This week addresses problems in\n\nThe object of using Synthetic Aperture Radar (SAR)\n\nDetecting changes in the Earth’s surface over time\n\nThe advantages of SAR for change detection\n\nsee through clouds\nhigh temporal resolution\n\nTechniques for change detection with SAR?\n\nratio\nlog ratios between two images\nt-tests\n\nfused with other data?\n\nYes, with optical data using techniques such as\n\nprincipal component analysis\nobject-based image analysis\nintensity fusion\n\n\nApplications?\n\nmonitoring land use changes\ndetecting deforestation\nidentifying urban growth pattern\n\n\n\n\n\nResolution and accuracy:\n\nUrban planning: Improved resolution and accuracy can influence local zoning regulations and urban growth management by providing detailed information on land use changes and the built environment. For example, high-resolution SAR data can be used to assess the effectiveness of urban containment policies or to identify areas where infrastructure investments are needed.\n\nData processing capabilities\n\nDisaster response: The ability to process larger datasets and monitor Earth’s surface in near real-time can inform global policies and agreements related to disaster management, such as the Sendai Framework for Disaster Risk Reduction. Rapid response to natural disasters, like earthquakes or hurricanes, can be coordinated more effectively with updated SAR data, allowing for quicker deployment of resources and better management of affected areas.\n\nNew SAR applications\n\nAgricultural: Advancements in SAR technology will expand its use in change detection and monitoring. This will support policies like the European Union’s Common Agricultural Policy (CAP), by providing data on crop health, irrigation needs, and land use changes.\nForestry (deforestation and reforestation tracking)\nDisaster response (flood and landslide monitoring)\nEnvironmental management: SAR data can inform policies related to wetland and coastal zone management, such as the Ramsar Convention on Wetlands and the United Nations Convention on the Law of the Sea (UNCLOS). By monitoring changes in these sensitive areas, policymakers can evaluate the effectiveness of existing regulations and develop new strategies to protect vital ecosystems.\n\nMachine learning and artificial intelligence:\n\nAdvanced algorithms will be able to identify and classify features and changes in SAR data analysis in the Earth’s surface, improving the overall accuracy of change detection, thus supporting climate change adaptation efforts at both local and global levels, including the United Nations Framework Convention on Climate Change (UNFCCC) and the Paris Agreement. Predictive models based on SAR data can help policymakers identify areas at risk of flooding, coastal erosion, or other climate-related impacts, enabling the development of targeted adaptation strategies.\n\n\nIn conclusion, advancements in SAR technology, combined with the integration of machine learning and artificial intelligence, will significantly enhance the capabilities for change detection and Earth surface monitoring. These developments will have far-reaching implications for various sectors, including agriculture, forestry, disaster management, and environmental protection, ultimately influencing policymaking and promoting more informed decision-making processes.\n\n\n\n\n\n\n\n\n\n\nTopic\nKey Points\n\n\n\n\nDefinition\nSynthetic Aperture Radar (SAR) is a type of radar that uses microwave signals to create high-resolution images of the Earth’s surface.\n\n\nAdvantages\nOperates in all weather conditions; penetrates through clouds and vegetation cover.\n\n\nLimitations\nSensitive to surface roughness; limited spatial resolution.\n\n\nProcessing Techniques\nInterferometry: combines multiple SAR images to create 3D maps of the Earth’s surface. Polarimetry: analyzes the polarization properties of reflected signals to extract additional information about surface features.\n\n\nApplications\nEnvironmental monitoring, disaster response, urban planning, military surveillance, and more.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nKey Points\n\n\n\n\nAdvantages of SAR for Change Detection\nCan see through clouds unlike optical sensors; high temporal resolution.\n\n\nChange Detection Techniques\nRatio or log ratios between two images; t-tests; standard deviation.\n\n\nFusion of SAR and Optical Data\nPrincipal component analysis; object-based image analysis; intensity fusion.\n\n\nApplications of Change Detection with SAR\nMonitoring land use changes, detecting deforestation, identifying urban growth patterns, and more."
  },
  {
    "objectID": "Week9.html#application",
    "href": "Week9.html#application",
    "title": "9  Week 9 - Synthetic Aperture Radar (SAR) data",
    "section": "9.2 Application",
    "text": "9.2 Application"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10  Summary",
    "section": "",
    "text": "1 + 1\n\n2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Google. 2023a. “Machine Learning in\nEarth Engine  Google\nEarth Engine.” Google\nDevelopers. https://developers.google.com/earth-engine/guides/machine-learning.\n\n\n———. 2023b. “Reducer Overview \nGoogle Earth Engine.”\nGoogle Developers. https://developers.google.com/earth-engine/guides/reducers_intro.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David\nThau, and Rebecca Moore. 2017. “Google Earth\nEngine: Planetary-Scale Geospatial Analysis\nfor Everyone.” Remote Sensing of Environment, Big\nRemotely Sensed Data: Tools,\napplications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nJensen, J. Robert. 1986. “Introductory Digital Image Processing: A\nRemote Sensing Perspective.” In.\n\n\nMacLachlan, Andrew, Eloise Biggs, Gareth Roberts, and Bryan Boruff.\n2021. “Sustainable City Planning:\nA Data-Driven\nApproach for Mitigating Urban\nHeat.” Frontiers in Built Environment 6. https://www.frontiersin.org/articles/10.3389/fbuil.2020.519599.\n\n\nNair, Rahul, P. J. Paul, and K. Poulose Jacob. 2018. “Automated\nImage Annotation: A Survey.” Journal of Imaging 4 (3):\n37.\n\n\nRobison, Keela. 2018. “The Future of Image Annotation: Human in\nthe Loop.” https://lionbridge.ai/articles/the-future-of-image-annotation-human-in-the-loop/.\n\n\nSaad El Imanni, Hajar, Abderrazak El Harti, El Mostafa Bachaoui, Hicham\nMouncif, Fatine Eddassouqui, Mohamed Achraf Hasnai, and Moulay Ismail\nZinelabidine. 2023. “Multispectral UAV Data for\nDetection of Weeds in a Citrus Farm Using Machine Learning and\nGoogle Earth Engine:\nCase Study of Morocco.” Remote\nSensing Applications: Society and Environment 30 (April): 100941.\nhttps://doi.org/10.1016/j.rsase.2023.100941.\n\n\nSchulte to Bühne, Henrike, and Nathalie Pettorelli. 2018. “Better\nTogether: Integrating and Fusing Multispectral and Radar\nSatellite Imagery to Inform Biodiversity Monitoring, Ecological Research\nand Conservation Science.” Methods in Ecology and\nEvolution 9 (4): 849–65. https://doi.org/10.1111/2041-210X.12942."
  }
]