[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Diary - CASA0023",
    "section": "",
    "text": "Introduction\nWelcome to my learning diary page of Remote Sensing Cities and Environment (CASA0023)! This diary is made for the content taught at 2022-2023.\nI’m a current Master of Science student at Bartlett Centre for Advanced Spatial Analysis.\nThis is a learning diary of a Master of Science student at CASA Module CASA0023 (Remote Sensing City Environment).\nThis learning diary is presented as a Quarto book containing 9 weeks as chapters.\nEach week, the content summarises that week’s teaching content in section summary, sometimes it emphasises comprehensiveness and therefore appear in overwhelming length. This way, it is easy for a general reader to get lost in finding what’s important. Therefore, I also added guides and highlighted what stood out in importance. Future development part can also assist in grasping the big picture of that week’s topic.\nApplication addresses one (or multiple) literature recommended in the module micro-site. It elaborates on parts that interest me and mentions why they are interesting. Also, the contribution and future literature advancement are highlighted.\nAs for reflection, I try to relate the content in Summary and Application to wider discipline in regard of how those can be of use in future both from my personal perspective, and in how a spatial data scientist might strengthen their arsenal using the content in dealing with Earth Observation data. The selection of content is largely based on how interesting they are, and hopefully the reason why they appear interesting has been illustrated in the their usefulness.\nTake a look at the module’s micro-site, to better understand what I’m talking about! CASA0023 Remotely Sensing Cities and Environments (andrewmaclachlan.github.io)\nThere are two weeks that differ in the general structure of this quarto book: Week 2 Portfolio includes only a Xaringan-made and online-hosted slide, Week 4 Policy deals with a policy instead of papers."
  },
  {
    "objectID": "Week1.html",
    "href": "Week1.html",
    "title": "1  Week 1 - Getting started with remote sensing",
    "section": "",
    "text": "Data types in remote sensing:\n\nPassive data: Energy in electromagnetic form (e.g., human eyes)\nActive data: Energy in addition to illumination (e.g., radar)\n\nInteraction of EM waves with Earth’s surface and atmosphere:\n\nSingle: This category refers to the interaction of EM waves with only one component of the Earth’s system, either the surface or the atmosphere. For example, when sunlight (an EM wave) directly reaches the Earth’s surface without interacting with the atmosphere, it is considered a single interaction. Similarly, if the EM wave only interacts with the atmosphere and does not reach the surface, it is also a single interaction. In remote sensing, single interactions are usually the most straightforward to analyze. The most straightforward to analyze.\nDual: Dual interactions involve the EM waves interacting with both the Earth’s surface and atmosphere. In this case, the EM waves pass through the atmosphere, interact with the Earth’s surface, and then pass back through the atmosphere before reaching the sensor. This process results in changes to the EM waves, such as absorption or scattering, which can affect the quality and interpretation of the remote sensing data. apply atmospheric correction techniques to retrieve accurate information about the Earth’s surface.\nQuad: Quad interactions refer to EM waves interacting with multiple components of the Earth’s system, such as the surface, atmosphere, and other features (e.g., vegetation, water bodies). Involve various processes such as absorption, scattering, reflection, and transmission. Account for the unique characteristics and properties of each component involved, making the process more challenging.\n\n\n\n\n\n\n\n\n\n\n\nInteraction Type\nComponents of Earth\nProcesses Considered\nDifficulty\n\n\n\n\nSingle\nSurface OR Atmosphere\nDirect interaction with one component\nMost straightforward to analyze\n\n\nDual\nSurface AND Atmosphere\nAbsorption, scattering\nRequires atmospheric correction for accurate results\n\n\nQuad\nSurface, Atmosphere, Features\nAbsorption, scattering, reflection, transmission\nMore challenging due to multiple components involved\n\n\n\n\n\n\nRaster formats:\n\nBIL\nBSQ\nBIP\nGeoTIFF\n\n\n\n\n\n\nSpatial resolution:\n\nRanges from 10 cm to several kilometers\n\nSpectral resolution:\n\nNumber of different spectral bands captured\nUnique spectral signatures for each feature on Earth\nAtmospheric windows\nVegetation: Red edge - infrared bands\n\nApplication: Analyze infrared bands in cities to identify access to vegetation\n\n\nRadiometric resolution:\n\nResolution of a cell’s value\n\nTemporal resolution:\n\nUsually inversely related to pixel size (spatial resolution)\n\n\n\nExample satellite sensor: MODIS\n\n\n\n\n\nIn remote sensing, bands refer to the specific ranges of wavelengths captured by a sensor.\nEach band captures information about different features on Earth’s surface.\nUnderstanding the properties of each band helps in the interpretation and analysis of remote sensing data."
  },
  {
    "objectID": "Week1.html#application",
    "href": "Week1.html#application",
    "title": "1  Week 1 - Getting started with remote sensing",
    "section": "1.2 Application",
    "text": "1.2 Application\nI explored Butcher (2016) for a better understanding of electromagnectic waves and the application in Remote Sensing.\n\n\n\n\n\n\n\n\n\nType of Wave\nWavelength Range\nFrequency Range\nExample Applications\n\n\n\n\nRadio Waves\n>1mm\n<300 GHz\nBroadcasting, communication, radar\n\n\nMicrowaves\n1mm - 1m\n300 MHz - 300 GHz\nCooking, communication, radar\n\n\nInfrared Waves\n700 nm - 1 mm\n300 GHz - 430 THz\nThermal imaging, remote controls\n\n\nVisible Light\n400 nm - 700 nm\n430 THz - 750 THz\nHuman vision, photography\n\n\nUltraviolet Waves\n10 nm - 400 nm\n750 THz -30 PHz\nSterilization, fluorescence microscopy\n\n\nX-Rays\n<10 nm\n>30 PHz\nMedical imaging, airport security\n\n\nGamma Rays\n<0.01 nm\n>30 EHz\nCancer treatment, nuclear medicine\n\n\n\n\n\n\nFigure 1.1: Spectral Feature Space, Vegetation On Bands B04 and B08\n\n\n“Spectral Feature Space, Vegetation On Bands B04 and B08”\nOne of the applications really attracted me was the spatial signature of vegetation on the terra, as we could assign features to each end of the spatial signature area see Figure 1.1, such as bare land on the right end of the triangle-like area where red light captured are dense while near-infrared level is low. Heavy vegetation are witnessed at the upper end of the triangle-like area where red light low and near-infrared is high, indicating heavy biomass. As for the left-down corner where both red and near-infrared are low, we can identify wet lands. This is integrated in the NDVI (Normalized Difference Vegetation Index) to estimate vegetation cover.\n\n\n\nFigure 1.2: workflow of the Electromagnetic Spectrum\n\n\nSpatial signatures can also be used to monitor the health of vegetation by identifying patterns of quavariation in spectral reflectance that are indicative of stress or disease. For example, vegetation that is stressed or diseased may have a different spectral reflectance signature than healthy vegetation, which can be identified using spatial signatures.\nIn addition, spatial signatures can be used to monitor the growth and distribution of vegetation over time by comparing satellite imagery from different dates. This can be useful for understanding the impacts of land use changes, climate change, and other factors on vegetation.\nOverall, spatial signatures are a powerful tool for vegetation monitoring, as they can be used to identify and classify different types of vegetation, monitor vegetation health, and track vegetation changes over time."
  },
  {
    "objectID": "Week1.html#reflection",
    "href": "Week1.html#reflection",
    "title": "1  Week 1 - Getting started with remote sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nHaving active sensing methods is inspiring as it reminds us that instead of struggling with improving image quality sensed passively using sunlight, we can try altering to artificial signals. Also that sunlight is but another form of electromagnetic wave. This implies a potential to cancel the boundary between natural phenomenon and artificial forms. I feel more encouraged to more actively explore relationships in nature with the devices at hand. I am considering implement active sensors to include SAR data for IoT-based data pipelines. With active sensing data like SAR monitoring forestry real-time, the machine learning model can acquire enough ground truth data from nature for constant adaptation. This could be incorporated into forestry monitoring systems and disaster monitoring systems to cover for both accuracy and rapidness.\nOne of the challenges I encountered is to navigate the complexities of the interface of SNAP and QGIS. It becomes clear to me that yes implementing several functions in code can be challenging, but a software with collective functions as a whole can be mindblowing even when with decent GUIs. Specifically, finding which function falling under which menu consumes a lot of time, and figuring out filling parameters to carry the analysis also took some efforts of iterative validation. Anyway, it’s nice to have attempts of designing GUIs for EO data manipulation. Hopefully, with Large Language Model simplifying GIS software designing, we can more easily translate code workflows into user-friendly interfaces and apply our design ideas.\n\n\n\n\nButcher, Ginger. 2016. Tour of the Electromagnetic Spectrum. Third edition. Washington, DC: National Aeronautics; Space Administration."
  },
  {
    "objectID": "Week2.html",
    "href": "Week2.html",
    "title": "2  Week 2 - Portfolio",
    "section": "",
    "text": "Xaringan operation instruction"
  },
  {
    "objectID": "Week3.html",
    "href": "Week3.html",
    "title": "3  Week 3 - Remote sensing data",
    "section": "",
    "text": "In this week’s learning diary, we try to handle"
  },
  {
    "objectID": "Week3.html#summary",
    "href": "Week3.html#summary",
    "title": "3  Week 3 - Remote sensing data",
    "section": "3.1 Summary:",
    "text": "3.1 Summary:\n\n3.1.1 Different Sensors\nAcross track scanners: Mirror reflects light onto 1 detector. For example, Landsat dataset are captured by this sort\nAlong track scanners: Basically several detectors pushed along. E.g., Quickbird, SPOT\n\n\n3.1.2 Geometric Correction\nRS data could include image distortions introduced by: View angle, topography, wind and rotation of the earth\nWe identify Ground Control Points (GCP) in distorted data to match them with local map, correct image, or GPS data from handheld device, but these reference images could also contain distortions and imprecisions.\nRMSE is adopted here to measure fitness between images. Use GCPs to minimise RMSE.\nDoing geometric correction can shift the original image, so we want to re-sample the final raster by using Nearest Neighbour, Linear, Cubic, Cubic spline re-samplers\n\n\n3.1.3 Atmosphric Correction\nAccording to Jensen (1986), two factors contribute to environmental attenuation: Atmospheric scattering, topographic attenuation.\nThere are unnecessary and necessary atmospheric corrections:\nnecessary ones are:\n\nBiophysical parameters needed (e.g. temperature, leaf area index, NDVI)\nE.g. .. .NDVI is used in the Africa Famine Early Warning System and Livestock Early Warning System\nUsing spectral signatures through time and space\n\nAbsorption and scattering can create the haze, i.e. reduces contrast of image.\nScattering can create the “adjacency effect”, radiance from pixels nearby mixed into pixel of interest.\n\n\n3.1.4 Orthorectification Correction\nThis is a subset of georectification, i.e. giving coords to an image. Particularly Orthorectification means removing distortion so pixels can appear being viewed at nadir (straight down). This requires the support of an Elevation Model to calculate the nadir view for each pixel on a sensor geometry.\nTo do this: cosine correction, Minnaert correction, Statistical Empirical correction, C Correction (advancing the Cosine). Need radiance (DN to TOA) from sloped terrain, Sun’s zenith angle, Sun’s incidence angle - cosine of the angle between the solar zenith and the normal line of the slope. Latter two found in angle coefficient files (e.g. Landsat data ANG.txt).\n\n\n3.1.5 Rdiometric Correction\nCorrections to raw satellite imagery can be performed using a method called Dark Object Subtraction (DOS). The logic is that the darkest pixel in the image should be 0 and any value it has is due to the atmosphere. To remove the atmospheric effect, the value from the darkest pixel is subtracted from the rest of the pixels in the image. The calculation involves converting the Digital Number (DN) to radiance, computing the haze value for each band (but not beyond NIR), and subtracting the 1% reflectance value from the radiance. The calculation requires values such as mean exoatmospheric irradiance, solar azimuth, Earth-sun distance, and others, which can be found in sources such as Landsat user manuals.\n\n\n3.1.6 Joining data sets\nAlso known as Mosaicking: We feather two images, creating a seamless mosaic, where the diving lien is called seamline.\n\n\n3.1.7 Image Enhancements\nImage stretch, Band ratioing, Normalised Burn Ratio, Edge enhancement, Filtering, PCA, Image fusion (see application) etc."
  },
  {
    "objectID": "Week3.html#application---discussing-image-fusion-in-one-literature",
    "href": "Week3.html#application---discussing-image-fusion-in-one-literature",
    "title": "3  Week 3 - Remote sensing data",
    "section": "3.2 Application - Discussing image fusion in one literature",
    "text": "3.2 Application - Discussing image fusion in one literature\n\nFrom literature we delve in the nuances of levels on which we perform image fusion to acquire better results. The integration methods vary as the levels vary (Schulte to Bühne and Pettorelli 2018).\nSatellite remote sensing (SRS) can be derived from Multispectral sensors and radar sensors. \n Multispectral sensors are passive, merely receiving electromagnetic waves reflected from surface, usually used to reflect chemical properties (such as nitrogen or carbon content and moisture). Usually produces data with comparatively low spatial resolution\n Radar ones emit electromagnetic radiation and measure the returning signal, responding to the three-dimensional structure of objects, being sensitive to their orientation, volume and surface roughness. Usually produces data with comparatively high spatial resolution\n\n3.2.1 Image fusion:\n1. decision-level (SRS integration), where separate predictors are used to estimate a parameter of interest. \n2. object-level (feature-level). unit: multi-pixel objects. (1) using radar and multispectral imagery is input into an object-based image segmentation algorithm, or (2) segmenting each type of imagery separately before combining them. multi-pixel objects\n3. pixel-level (Observation-level), where pixel values are combined to derive a fused image with new pixel values, either in the spatial or the temporal domain.\n(2. and 3. derive entirely new predictors.)\n\n\n\nFigure 3.1: Credit: Schulte to Bühne and Pettorelli (2018)\n\n\nSchematic overview of multispectral-radar SRS data fusion techniques. The parameter of interest can be a categorical variable, like land cover, or a continuous variable, like species richness. In pixel-level fusion, the original pixel values of radar and multispectral imagery are combined to yield new, derived pixel values. Object-based fusion refers to (1) using radar and multispectral imagery is input into an object-based image segmentation algorithm, or (2) segmenting each type of imagery separately before combining them. Finally, decision-level fusion corresponds to the process of quantitatively combining multispectral and radar imagery to derive the parameter of interest (by e.g. combining them in a regression model, or classification algorithm)\n\n\n3.2.2 Implementation Approaches\n\n\n\nFigure 3.2: Credit: Schulte to Bühne and Pettorelli (2018)\n\n\npixel-level\n\nComponent substitution techniques: such as principal component analysis (PCA), Intensity-hue-saturation (IHS). \n\nPCA is the only pixel-level image fusion technique that cannot be applied to imagery with different spatial resolutions, and the only that allows unlimited image numbers.\n\nIHS fusion. Three images with lower spatial resolution (typically multispectral data) are integrated with a single image with high spatial resolution (typically radar) to retain the radiometry but increase the spatial resolution of the former. Facilitate visual interpretation by combining resulting images into a single RGB image.\n\nMulti-resolution analysis, such as **Wavelet transformation. Decompose multispectral and radar imagery into their respective low- and high-frequency components\n\nArithmetic fusion techniques: such as the Brovey transform algorithm. Unlikely to be appropriate for multispectral-radar SRS image fusion.\n\nObject-level: Based on brightness and intensity values of each pixel, as well as its spatial context, objects such as lines, shapes or textures are extracted.\n1. image segmentation: Demands that multispectral and radar SRS images are with the same spatial resolution\n2. *extracting objects separately and combining in a feature map*\nObject-based fusion reduces all multispectral and radar information into a single layer of discrete objects, which are often relatively easy to relate to ecological features.\nDecision-level fusion: Quantitative decision-making frameworks—such as a regression, a quantitative model or a classification algorithm."
  },
  {
    "objectID": "Week3.html#reflection",
    "href": "Week3.html#reflection",
    "title": "3  Week 3 - Remote sensing data",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nData correction, Data fusion and Image enhancement SRS data fusion can increase the quality of SRS (Satellite Remote sensing)-derived parameters for application in terrain detection, urban analysis, ecology and conservation (Schulte to Bühne and Pettorelli 2018). It is thus important to explore how best to capitalise on recent technological developments and changes in SRS data availability. It is exctiing to apply solid machine learning methods to this area and it is marvelous to see the progress reflected by the increasing number of software supporting this application. The improvement of image quality enables new research designs in ecology and conservation areas and reignite previously greyed-out options.\nThe application of data correction, data fusion, and image enhancement techniques to SRS data can greatly improve the accuracy and reliability of SRS-derived parameters, which can then be used in various fields, including terrain detection, urban analysis, ecology, and conservation. With the rapid advancements in technology and the increasing availability of SRS data, there is a growing opportunity to leverage the latest machine learning techniques in this area. The development of new software tools to support these applications is a testament to the progress being made in this field. By enhancing the quality of the SRS data, researchers are able to design more robust and informative studies, unlocking new insights and avenues for exploration in ecology and conservation. This, in turn, has the potential to lead to breakthroughs and innovations in these fields, making a significant impact on the world around us.\n\n\n\n\nJensen, J. Robert. 1986. “Introductory Digital Image Processing: A Remote Sensing Perspective.” In.\n\n\nSchulte to Bühne, Henrike, and Nathalie Pettorelli. 2018. “Better Together: Integrating and Fusing Multispectral and Radar Satellite Imagery to Inform Biodiversity Monitoring, Ecological Research and Conservation Science.” Methods in Ecology and Evolution 9 (4): 849–65. https://doi.org/10.1111/2041-210X.12942."
  },
  {
    "objectID": "Week4.html",
    "href": "Week4.html",
    "title": "4  Week4 - Policy Case Study in",
    "section": "",
    "text": "OneNYC-2050-Summary.pdf (cityofnewyork.us)"
  },
  {
    "objectID": "Week4.html#summary",
    "href": "Week4.html#summary",
    "title": "4  Week4 - Policy Case Study in",
    "section": "4.1 Summary",
    "text": "4.1 Summary"
  },
  {
    "objectID": "Week4.html#application",
    "href": "Week4.html#application",
    "title": "4  Week4 - Policy Case Study in",
    "section": "4.2 Application",
    "text": "4.2 Application\nThe initiatives provided cover a wide range of areas, including education, small business support, community resilience, infrastructure, transportation, and sustainability. One of the key applications of these initiatives is to improve the quality of life for New Yorkers, particularly those in underrepresented communities. For example, the initiatives aimed at increasing the number of New Yorkers earning a high school equivalency diploma and connecting underrepresented groups to construction jobs created by City investments are designed to provide greater economic opportunities and upward mobility.\nSimilarly, the initiatives aimed at enhancing walkability and accessibility and improving the sustainability and efficiency of air travel are designed to improve the physical infrastructure of the city and make it more accessible and sustainable for all residents. Another application of these initiatives is to promote equity and inclusivity in the city.\nMany of the initiatives are specifically targeted at underrepresented communities, such as the initiatives aimed at providing paid internships and professional development opportunities to cultural workers and supporting the growth and retention of small businesses.\nBy providing resources and support to these communities, the city aims to promote greater equity and inclusivity and reduce disparities in access to resources and opportunities.\nThe initiatives also reflect a commitment to sustainability and resilience. Many of the initiatives are aimed at improving the city’s infrastructure and transportation systems to make them more sustainable and resilient in the face of climate change and other challenges. For example, the initiatives aimed at investing in innovative and resilient transportation networks and enhancing walkability and accessibility are designed to reduce emissions and congestion and promote sustainable modes of transportation.\nOverall, the initiatives outlined in the text reflect a comprehensive and multi-faceted approach to improving life in New York City. While there is still much work to be done, these initiatives represent an important step forward in promoting equity, sustainability, and resilience in the city. As future literature advancements are made, it will be important to continue to evaluate and refine these approaches to ensure that they are effective and responsive to the needs of all New Yorkers."
  },
  {
    "objectID": "Week4.html#reflection",
    "href": "Week4.html#reflection",
    "title": "4  Week4 - Policy Case Study in",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nThe initiatives outlined in the text cover critical components of urban planning and development, including infrastructure, transportation, education, and sustainability. The initiatives outlined in the text emphasize the importance of investing in the city’s data infrastructure and establishing a citywide data catalog, among other things. This requires a deep understanding of data management and analysis techniques, as well as the ability to work with large and complex datasets. Spatial data scientists and deep learning solution engineers can use these skills to develop innovative solutions for urban planning and development, such as predictive models for traffic flow or energy consumption.\nMany of the initiatives outlined in the text involve the use of GIS products and services, as well as the expansion of walkability and accessibility in the city. This requires a deep understanding of geospatial data analysis techniques, as well as the ability to work with mapping and visualization tools. Spatial data scientists and deep learning solution engineers can use these skills to develop innovative solutions for urban planning and development, such as interactive maps that show the most walkable routes in the city.\nIn addition to these skills, the content, data, and tools presented in the text are highly relevant to the broader discipline of urban planning and development. Spatial data scientists and deep learning solution engineers can use these resources to develop innovative solutions for a wide range of urban challenges, from improving transportation networks to promoting sustainability and resilience. By leveraging these resources, these professionals can help to create more livable and equitable cities that meet the needs of all residents. Looking to the future, it will be important for spatial data scientists and deep learning solution engineers to continue to stay up-to-date with the latest advancements in data management, analysis, and visualization techniques."
  },
  {
    "objectID": "Week5.html",
    "href": "Week5.html",
    "title": "5  Week 5 - An introduction to Google Earth Engine",
    "section": "",
    "text": "This week introduces Google Earth Engine (GEE), a geospatial processing service that allows for planetary scale analysis of massive datasets in seconds.\nBasics:\nObjects and methods in GEE are introduced:\nAlso, the types of analyses that can be performed in GEE are briefly covered."
  },
  {
    "objectID": "Week5.html#summary",
    "href": "Week5.html#summary",
    "title": "5  Week 5 - An introduction to Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nIntroduced GEE Basics, Objects, Geometries and applications.\nTable 1: Terms, Jargon, and Processes Related to Google Earth Engine\n\n\n\n\n\n\n\n\nCategory\nTerm/Aspect\nDefinition\n\n\n\n\nBasics\nGoogle Earth Engine\nA geospatial processing service that allows geospatial analysis at scale.\n\n\nBasics\nImage\nRefers to raster data in GEE and has bands.\n\n\nBasics\nFeature\nRefers to vector data in GEE and has geometry and attributes.\n\n\nBasics\nImageCollection\nA stack of images in GEE.\n\n\nBasics\nFeatureCollection\nA stack of features (lots of polygons) in GEE.\n\n\nBasics\nProxy objects\nGEE objects that are stored on the server and have no data in the script.\n\n\nObjects\nEarth Engine Objects\nObjects in GEE starting with “ee”.\n\n\nObjects\nImages (Rasters)\nGEE object representing raster data with bands.\n\n\nObjects\nFeature\nGEE object representing vector data with geometry and attributes.\n\n\nObjects\nImageCollection\nA stack of images in GEE.\n\n\nObjects\nFeatureCollection\nA stack of features (lots of polygons) in GEE.\n\n\nObjects\nJoins\nCombining two FeatureCollections with a common property.\n\n\nObjects\nArrays\nUsed to store and manipulate lists of values.\n\n\nObjects\nChart\nUsed to visualize data in GEE.\n\n\nGeometry\nPoint\nA single location represented by its longitude and latitude.\n\n\nGeometry\nLine\nA series of connected points representing a linear feature.\n\n\nGeometry\nPolygon\nA closed shape with three or more sides, represented by a series of connected lines forming a closed loop.\n\n\nGeometry\nMultiPolygon\nA collection of polygons, where each polygon is represented as a list of coordinate tuples defining its vertices.\n\n\nGeometry\nMultiGeometry\nA collection of different types of geometries.\n\n\nProcesses\nReducing\nSummarizing data over a specified dimension or property.\n\n\nProcesses\nFiltering\nReducing data to a specific subset based on a specified condition.\n\n\nProcesses\nMapping\nApplying a function to every element of a collection in GEE.\n\n\nProcesses\nScaling\nRefers to the pixel resolution in GEE. The scale is set by the output, not the input, and Earth Engine selects the pyramid with the closest scale to analysis.\n\n\nClient/Server\nClient Side\nRefers to the browser side of GEE.\n\n\nClient/Server\nServer Side\nRefers to the side of GEE where data is stored.\n\n\nClient/Server\nLooping\nLooping is not recommended for objects on the server side.\n\n\nClient/Server\nMapping\nInstead of loops, mapping is used in GEE to apply a function to everything on the server.\n\n\n\n\n5.1.1 GEE Basics\nJavaScript, where objects are dictionaries:\n\nWe have ee (EarthEngine), a powerful package. Anything starting with ee (proxy objects) are stored on the server.\nProblems:\n\nWe don’t iterate the data on the server; instead, we map (using a mapping function) them into objects (variables) so we only load them once.\nThere are also some sort of server-wide functions.\nAvoid using loops in GEE on the server-side, as mapping can automatically detect the number of loops needed.\n\n\nScale:\n\nPixel resolution, set by the output.\nGEE does resampling, aggregating your input to a 256*256, mainly down-sampling.\n\n\n\n5.1.2 GEE Objects\nObjects:\n\nImages (Rasters), geometry, ImageCol, features, featureCol, joins, arrays, chart.\n\n\n\n5.1.3 GEE Processes and Applications/Outputs\nGEE applications:\n\nReducing types.\nDifferent to filterBounds() that filters the area of interest, to do zonal statistics, we have reduceRegion(), where regions are subcategories of the area of interest.\nAlso, we have reduceNeighborhood(), which is a bit like a kind of image enhancement.\n\nLinear Regressions:\n\nIn a scenario of visualising precipitation, we can do a multivariate multiple linear regression where both independent variables (time) and dependent (precip, temp) variables are multiple.\nSomething about constant bound.\n\nJoins:\n\nIn GEE, everything, e.g. within a buffer, intersect, etc. needs the mediation of Join (apply()).\nTo perform joins, we need to put data into Filter().\n\nClassifiers:\n\nPer-pixel\nsub-pixel\n\nTable 2: GEE Processes and Applications/Outputs\n\n\n\n\n\n\n\nProcess\nDescription\n\n\n\n\nGeometry operations\nSpatial operations such as union, intersection, buffer, and distance analysis\n\n\nJoins\nCombining two feature collections based on a shared attribute value\n\n\nZonal statistics\nComputing statistics for a region or set of regions such as mean, median, and mode of pixel values within a feature or a collection of features\n\n\nFiltering\nFiltering of images or specific values based on criteria such as date range, location, and attribute value\n\n\nMachine learning\nUsing statistical and machine learning algorithms for classification, clustering, and prediction tasks\n\n\nDeep learning\nA subset of machine, using Deep Neural Networks\n\n\n\n\n\n5.1.4 Advantages and Limitations\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\n1. Large-scale data processing\n1. Limited to Google’s data catalog\n\n\n2. Access to vast satellite imagery library\n2. Steeper learning curve for beginners\n\n\n3. Real-time data analysis capabilities\n3. Requires coding skills (JavaScript, Python)\n\n\n4. Cloud-based platform\n4. Limited customization options\n\n\n5. Free for non-commercial use\n5. Data export restrictions\n\n\n6. Easy data sharing and collaboration\n6. Dependent on internet connectivity\n\n\n\n*No support for phase data, needs SNAP.\n\n\n5.1.5 Trend\nSee also Section Application for details and references.\n\nEnhancement user interface: GEE might introduce a more user-friendly interface to lower the entry barrier for beginners and non-programmers, making it more accessible to a wider audience.\nIntegration with machine learning and AI: GEE could expand its integration with advanced machine learning and AI algorithms, enabling users to derive more sophisticated insights from geospatial data.\nCustomisable solutions: GEE may introduce more customization options for users, allowing them to develop tailored geospatial analysis tools and applications.\nBetter support for commercial use: GEE could offer more comprehensive support and licensing options for commercial users, helping businesses harness the full potential of geospatial data analysis."
  },
  {
    "objectID": "Week5.html#application",
    "href": "Week5.html#application",
    "title": "5  Week 5 - An introduction to Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\nLiterature choice: Gorelick et al. (2017).\nThis week’s recommended literature mainly are documentation support for GEE and literature, even papers. Therefore the contribution of the literature will be in more general senses. An overview of Google Earth Engine’s capabilities and applications, as well as its potential to address societal issues.\nThey also discuss potential future developments, including expanding Earth Engine’s data catalog, improving its user interface, and increasing collaboration with other organizations.\n\nExpanding Earth Engine’s data catalog: currently includes a wide range of geospatial datasets but could be expanded to include additional sources of data (Gorelick et al. 2017).\nImproving the user interface: make it more intuitive and user-friendly, particularly for non-expert users.\nIncreasing collaboration with other organizations: Collaboration with other organizations, both in terms of data sharing and joint research projects, is also an important area for future development.\nOngoing research into new analysis techniques and algorithms: Ongoing research into new analysis techniques and algorithms will continue to expand Earth Engine’s capabilities and applications (Moore and Hansen 2011)."
  },
  {
    "objectID": "Week5.html#reflection",
    "href": "Week5.html#reflection",
    "title": "5  Week 5 - An introduction to Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nGEE-using skills can be a valuable asset for a spatial data scientist, as it allows for complex spatial analysis at scale. Traditional GIS software are eclipsed when it comes to both efficiency and scale.\nGEE’s unique and efficient way of conducting analysis flows is interesting, such as the introduction of concepts like client vs server-side operations and data reduction techniques. These was required by GEE’s feature of carrying out analyses on massive datasets (Gorelick et al. 2017). For those interested in BigData technology, the strategies (server/client split, no looping on server, etc.) applied by Google here is a very resourceful one and worth learning. The user end also has to learn to adopt good practices for reducing data range, which has been simplified to a series of reduction and filtering functions, e.g. ImageCollection.filterDate(), image.reduceNeighborhood()(Google 2023b).\nGEE’s combination with machine learning is also promising in regard of automating complex analysis tasks, as Machine Learning APIs offered by GEE support Supervised and Unsupervised Classification, and Regression (Google 2023a). According to Saad El Imanni et al. (2023), as a subtask of intelligent agriculture, weeds detection task sees an impressive performance (overall accuracy reached 96.87%) when GEE and Machine learning are combined.\nIn conclusion, I believe familiarity with GEE will add to one’s machine learning workflow in dealing with EO data, and, more generally, incredibly large datasets. The design of GEE also opens an era of web-service based big-data handling. Its designs in alleviating computation on client side and getting rid of for-loop on server-side inspires service designers to make distinct standards for code practice based on the server-client split. Besides, the sheer amount and diverse categories of data available on GEE saves experts from burdensome data collection process, so they can focus more on EO data processing, analysis and storytelling.\n\n\n\n\nGoogle. 2023a. “Machine Learning in Earth Engine  Google Earth Engine.” Google Developers. https://developers.google.com/earth-engine/guides/machine-learning.\n\n\n———. 2023b. “Reducer Overview  Google Earth Engine.” Google Developers. https://developers.google.com/earth-engine/guides/reducers_intro.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-Scale Geospatial Analysis for Everyone.” Remote Sensing of Environment, Big Remotely Sensed Data: Tools, applications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nMoore, R., and M. Hansen. 2011. “Google Earth Engine: A New Cloud-Computing Platform for Global-Scale Earth Observation Data and Analysis.” AGU Fall Meeting Abstracts, December, 02.\n\n\nSaad El Imanni, Hajar, Abderrazak El Harti, El Mostafa Bachaoui, Hicham Mouncif, Fatine Eddassouqui, Mohamed Achraf Hasnai, and Moulay Ismail Zinelabidine. 2023. “Multispectral UAV Data for Detection of Weeds in a Citrus Farm Using Machine Learning and Google Earth Engine: Case Study of Morocco.” Remote Sensing Applications: Society and Environment 30 (April): 100941. https://doi.org/10.1016/j.rsase.2023.100941."
  },
  {
    "objectID": "Week6.html",
    "href": "Week6.html",
    "title": "6  Wk6 Classification",
    "section": "",
    "text": "Information\nSummary\n\n\n\n\nPurpose of classification\nTo subset data into classes or values, such as landcover or estimating values like GCSE scores or pollution.\n\n\nDifferent classification methods\nEssentially slice the data in different ways.\n\n\nComplexity of classification methods\nThey can often be made to appear more complicated than they are.\n\n\nControlling classifiers\nCan be done using hyperparameters.\n\n\nDesired outcome of classifiers\nCan range from a single tree to a decision hyperplane boundary in multiple dimensions.\n\n\n\n\n\nTable 1: Supervised Classification Methods\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nMaximum Likelihood\nA statistical method used to estimate the parameters of a probability distribution based on observed data.\n\n\nSupport Vector Machines (SVM)\nA supervised learning algorithm that finds the best hyperplane to separate data into different classes.\n\n\n\nTable 2: Unsupervised Classification Methods\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nDensity Slicing\nDivides the range of pixel values into equal intervals and assigns each interval a unique class value.\n\n\nParallelpiped\nUses a set of user-defined ranges for each band to define class boundaries in multi-dimensional space.\n\n\nMinimum Distance to Mean\nAssigns each pixel to the class with the closest mean value in multi-dimensional space.\n\n\nNearest Neighbor\nAssigns each pixel to the class of its nearest neighbor in multi-dimensional space.\n\n\n\nTable 3: Other Machine Learning Methods\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nArtificial Neural Networks (ANN)\nA set of algorithms inspired by the structure and function of biological neural networks, used for pattern recognition and prediction tasks.\n\n\n\n\n\n\nTable 1: Supervised vs. Unsupervised Classification\n\n\n\n\n\n\n\n\nClassification Type\nDefinition\nMethod\n\n\n\n\nSupervised\nClassifier learns patterns in the data and uses that to place labels onto new data. Pattern vector is used to classify the image. Usually, pixels are treated in isolation but as we have seen - contextual (neighboring pixels), objects (polygons), texture.\nPattern recognition or machine learning\n\n\nUnsupervised\nIdentifies land cover classes that aren’t known a priori (before) and tells the computer to cluster based on info it has (e.g. bands) and label the clusters.\nDensity slicing, parallelpiped, minimum distance to mean, nearest neighbor, neural networks, machine learning / expert systems*\n\n\n\n\n\n\n\nBias refers to the difference between the predicted value and the true value. When a model has high bias, it is too simple and may underfit the data. On the other hand, when a model has low bias, it may overfit the data.\nVariance, on the other hand, refers to the variability of a model for a given point. When a model has high variance, it is too complex and may overfit the data. This means that it will perform well on the training data but poorly on new data.\n\n\n\n\nFigure 6.1: Credit: CASA0006\n\n\nIn general, overfitting occurs when there is a trade-off between bias and variance. A model with high bias and low variance will underfit the data, while a model with low bias and high variance will overfit the data. The goal is to find a balance between bias and variance that results in good performance on both training and test data.\n\n\n\nFigure 6.2: Credit: CASA0006\n\n\n\n\n\nEarth Observation (EO) data classification is continually evolving, with new technologies and techniques leading to several anticipated future developments:\n\nMulti-source data fusion:\n\nIntegrating data from multiple sources like satellite imagery, LiDAR, and ground-based sensors will become more prevalent. This fusion enhances classification accuracy and offers comprehensive Earth’s surface information, improving decision-making and monitoring. For example, the European Union’s Copernicus Programme could use this in providing free data from various satellite missions and sensors for environmental monitoring, disaster management, and urban planning.\n\nMulti-temporal analysis:\n\nSophisticated multi-temporal analysis techniques will be increasingly used to monitor changes in land cover, vegetation, and other features over time. This enables accurate and efficient change detection and monitoring of phenomena like urbanization, deforestation, and climate change. This aligns with the REDD+ initiative under the United Nations Framework Convention on Climate Change (UNFCCC), which uses multi-temporal analysis to monitor forest cover changes and evaluate policy effectiveness in reducing greenhouse gas emissions from deforestation and forest degradation.\n\nCloud-based processing:\n\nThe growth of cloud-based platforms, such as Google Earth Engine, allows for more efficient and scalable EO data processing workflows. This accessibility enables researchers and organizations to innovate in classification techniques and applications. The National Oceanic and Atmospheric Administration’s (NOAA) Big Data Project aims to make vast amounts of environmental data accessible and usable in the cloud, fostering innovation in developing new applications and services.\n\n\nThese advancements will provide comprehensive and timely information about Earth’s surface, informing policies and strategies in areas such as environmental management, disaster response, and urban planning."
  },
  {
    "objectID": "Week6.html#application---support-vector-machines-for-classification-in-remote-sensing",
    "href": "Week6.html#application---support-vector-machines-for-classification-in-remote-sensing",
    "title": "6  Wk6 Classification",
    "section": "6.2 Application - ****Support vector machines for classification in remote sensing****",
    "text": "6.2 Application - ****Support vector machines for classification in remote sensing****\n\nDeep Learning methods can have universally good performance across Computer Vision tasks, e.g. Earth Observation data classification, not to mention techniques like transfer learning (pretrained model plus large data for a specific task) and meta learning (large universal pretrained model plus small amount of task-specific data) can further strengthen the accuracy.\nHowever, in this week’s literature [https://www.notion.so/Wk6-Classification-98918c59eebc4b869f45ec22d1529657?pvs=4#b66310f96d47495f94efc9715bc4c9f2], SVM was demonstrated to generate better result with smaller data amount.\nThis vastly contributes to the particular task of remote sensing image classification. Also, we can derive insights of how elegant choice of model (SVM in this case) for downstream tasks can outperform blindly stacking (make ensemble of) popular neural networks.\n\n6.2.1 Support Vector Machine\nThis model basically attempts at maximising margins of fitting lines that are trying to classify points. To do this, it finds an optimal hyperplane (”lines” extended in dimensionality). In the sense that it projects data into higher dimensions, it resembles kernel methods. Some even categorise SVM as one of kernel methods.\nOh, higher dimensions! Sounds computation-intense? But this is already an alleviation of computation compared to Neural Networks.\nBesides, the amount of required data and scale of model weights are severely reduced, making it easier for both algorithm engineers to train and Remote Sensing experts to use.\n\n\n6.2.2 Rationale Behind the Paper\nThe paper here deals with small amount of data, with ground truth selected using a random sampling procedure. To effectively use small data, it adopts SVM and achieved high classification accuracy with high dimensional data.\nThe author also delves into the detailed problems encountered using SVM. For classification task, two-class or multi-class problems are separately discussed. Usually, Earth Observation data falls within the multi-class one. The ‘one against one’ and the ‘one against the rest’ strategies for generating multi‐class SVMs are compared in this study. The “one-against-one” method proved to be better for multi-class.\nRemote-sensing data often have different spectral, spatial and temporal resolutions, which pose challenges for traditional classification methods. SVM can overcome these challenges by mapping the data into a higher-dimensional feature space where a linear separator can be found. This way, It can\n\nhelp identify and map different land cover types and changes over time\nassist in monitoring and managing natural resources, such as forests, water, soil, etc.\nprovide valuable information for disaster management, such as flood detection, fire risk assessment, landslide susceptibility, etc.\nsupport various applications in agriculture, urban planning, climate change studies, biodiversity conservation, etc.\nreduce the cost and time of field surveys and data collection\n\n\n\n6.2.3 Future Advancement\n\nCombing ANN and SVM:\n\nA hybrid regression support vector machine-convolutional neural network (HRSVM-CNN) classifier can be used for object-based classification of high-resolution remote sensing images [Full article: Object based classification of high resolution remote sensing image using HRSVM-CNN classifier (tandfonline.com)]. In this approach, the image data is preprocessed and segmented, and then features are extracted using techniques such as local ternary patterns (LTrP), color histograms, gray-level co-occurrence matrices (GLCM), gray-level difference method (GLDM), edge features, and shape features. These extracted features are then classified using the HRSVM-CNN classifier.\n\nSVM can go further in its advantages:\n\nTransfer learning involves using a pre-trained model that has been trained on a large dataset to extract features from the data. These extracted features can then be used as input to an SVM classifier trained on a smaller dataset [TL-SVM: A transfer learning algorithm | Semantic Scholar]. This approach can take advantage of the ability of the pre-trained model to learn complex representations of the data and the ability of SVMs to find good decision boundaries even when the data is not linearly separable.\nActive learning, which involves iteratively selecting the most informative samples from a pool of unlabeled data and adding them to the training set. This can help to improve the performance of an SVM classifier even when the amount of labeled data is very limited."
  },
  {
    "objectID": "Week6.html#reflection",
    "href": "Week6.html#reflection",
    "title": "6  Wk6 Classification",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nThis week, I have been absent from the lecture and workshop, because I was at Data Dive CUSP London, which is quite an opportunity for meeting people in data science sector and honing skills of utilising data science skills to tell a story addressing real-world problems. My group explored “Does built-environment have an influence on Londoners’ mental health”, where we tried to utilise latest deep learning methods like attention mechanism and U-map for analysis and visualisation for a high ‘technical complexity’ mark.\nI have been obsessed with Neural Networks during my undergraduate years: How can I distill this General Pretrained Model to be locally implementable to be my personal poem-composing assistant? How can I deploy this open-source Object Detection model on an ARM (Advanced RISC Machines) built in an IoT camera to detect high-street footfall? But the SVM (Support vector machines) introduced in this week’s literature really proves how simpler models (without neural classifiers @Benediktsson et al., Tso and Mather) can achieve performance no worse than the SOTA Deep Learning models in specific tasks like ****classification in remote sensing****.\nThis insights, elegant choice of model (SVM in this case) for downstream tasks can outperform blindly stacking (make ensemble of) popular neural networks, unveil new potentials for me when optimsing model performance, e.g., when doing transfer learning on a pretrained model on a classification task, I might consider experimenting with SVM in parallel with hyper-tuning Neural network, and seek possibilities of combining the two.\nDespite the usefulness of this diversity of classification models, always\n\nPay attention to their assumptions,\nCheck carefully if our data and problem align with these assumptions.\n\nIf not, process accordingly to satisfy them or switch methods.\n\n\nEspecially, when combining different Machine Learning methods, e.g. in module GISS(CASA0005), we used the result of KNN models to decide parameters (min_point and radius) for DBSCAN, always look into data to ensure the assumptions are met. The disparity in alignment with model assumptions can have impact on the whole data pipeline."
  },
  {
    "objectID": "Week7.html",
    "href": "Week7.html",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "",
    "text": "This week’s learning diary continues that from Week 6 in addressing the big problem in Remote Sensingm, i.e. classification within Earth Observation data. Also, accuracy metrics are discussed."
  },
  {
    "objectID": "Week7.html#summary",
    "href": "Week7.html#summary",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nThe summary of lecture content as well as practical outcomes. See ?fig-mindmap for an overview. If certain words are intelligible due to resolution issues, hopefully you can right click and “open in new page” to get a better view since this is a .SVG file. \n\n7.1.1 Data\n\nSurface Reflectance (SR)\nTop of Atmosphere (TOA)\n\nA mixed way of doing urban recognition\n\n\n7.1.2 OBIA (object-based image analysis)\nInstead of a per-pixel approach, we adopt an object-based image analysis (OBIA), where you have to manually create objects.\nSLIC (Simple Linear Iterative Clustering) (2012): No ground truth\nDescent, similarity (Homogeneity)\n\n\n7.1.3 Sub-pixel analysis\nSMA (Spectral maximum analysis), SPC, LSU\nThrough a series of manipulation of material, we acquire a list describing the broken-down land cover of that pixel\n\n7.1.3.1 Pixel purity\nEndmember: an important concept in spectral mixture analysis\nIn remote sensing, an end member refers to a pure or nearly pure material or component that is present within a mixed pixel.\nIn spectral mixture analysis, the spectral signature of a mixed pixel is modelled as a linear combination of the spectral signatures of the constituent endmembers, with each end member being assigned a proportion or fraction that represents its contribution to the overall reflectance or radiance of the mixed pixel.\n\n\n\n7.1.4 Accuracy assessment\n\nProducer accuracy: Recall\nUser’s accuracy: Precision\nOverall accuracy: not equivalent to F1\nKappa coefficient: [0, 1], measures how good the classification is compared to random distribution e.g. Poisson. Different interpretations of this metric, problematic\n\nMake a tradeoff between Producer accuracy and User accuracy, by shifting the decision boundary.\n\nF1: issue: TN not considered; Recall and precision are not equally important yet equally weighted in F1\nReceiver Operating Characteristic Curve: True positive rate and false positive rate are all good. We want to maximise the area under the curve in a True positive rate vs false positive rate plot.\n\n\n\n7.1.5 Workflow\n\n(Potentially use unsupervised classification to understand your data\nClass definition (Potentially use unsupervised classification)\nPreprocessing\nTraining\nPixel Assignment\nAccuracy assessment\n\nPseudo-invariant features to be trained on to make your model robust to time-space changes\nPseudo-invariant features are often used as reference targets or calibration sites in remote sensing to account for changes in sensor or atmospheric conditions and to reduce the effects of noise and calibration drift on image data. These features have relatively constant spectral properties over time and space, and can therefore serve as a stable reference for monitoring changes in other features or materials within an image or scene.\nA flow chart can be seen in Figure 7.1 :\n\n\n\nFigure 7.1: Classification Workflow, courtesy: myself\n\n\n\n\n7.1.6 A “Sneak preview” (Analogous to Data Leakage in ML)\nWaldo Tobler’s first law of geography indicates that if training and testing are spatially close, the training can cause the problem of a sneak preview.\n\n7.1.6.1 Spatial Cross Validation\nSimilar to cross-validation but adds clustering to folds.\nIn spatial cross-validation, the data are split into spatially contiguous blocks or subsets, rather than randomly shuffled subsets as in traditional cross-validation. This is done to ensure that the model is tested on data that are spatially distinct from the data used to train the model and to account for spatial autocorrelation and other spatial dependencies in the data.\n\n\n\n7.1.7 Approaches to deal with Spatial Autocorrelation\nObject-based image classification\nMoran’s I (Spatial Cross Validation)"
  },
  {
    "objectID": "Week7.html#application---to-be-completed",
    "href": "Week7.html#application---to-be-completed",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.2 Application - to be completed",
    "text": "7.2 Application - to be completed\nIn remote sensing, it is often challenging to accurately classify mixed pixels, which contain a combination of different materials or components. Endmembers refer to pure or nearly pure materials that are present within a mixed pixel. By modelling the spectral signature of a mixed pixel as a linear combination of the spectral signatures of the constituent endmembers, we can determine the contribution of each endmember to the overall reflectance or radiance of the mixed pixel.\nThis approach can be very useful in urban recognition, where it is essential to accurately classify the different land covers present within a pixel. Furthermore, it can also help us to understand the composition of the land cover in a given area, which can have important implications for environmental monitoring and management. This approach has been applied in various studies to estimate urban land cover, such as the work by Zhang et al. (2018) that utilised endmember extraction to detect urban impervious surfaces."
  },
  {
    "objectID": "Week7.html#reflection",
    "href": "Week7.html#reflection",
    "title": "7  Week7 - Classification and Accuracy",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nThe workflow of Classification of Surface Reflectance and Top of Atmosphere data intrigues me, as it differs from, yet shares certain features with traditional computer vision tasks like image classification and object detection (in regards of treating pixels as objects/units). Alternatively, Surface Reflectance Classification can be treated as a downstream task for both aforementioned ML tasks, due to its uniqueness in dealing with high-precision satellite data and unseparability (worth debating) from EO processing workflow (calibration etc.). Also, uncertainty of classification genres derived from unsupervised labelling can also be an issue.\nOptionally, in supplementation to manual labelling, automated labelling workflow (e.g. roboflow) can be introduced to curtail repeatitive works in image labelling(Nair, Paul, and Jacob 2018). However, manual labelling is not replaceable at current time (Robison 2018) and the pinning down of ground truth seems to always need human intervention in addition to machine automation.\nThe introduction of production accuracy and user accuracy is also interesting, as these terminologies are designed presupposing a customer/producer split, dwarfing precision-recall in readability. The treadeoff to be made between the two is crucial, and this is problematically handled by introducing F1 score with two competitive components, and improved by introducing Receiver Operating Characteristic Curve (ROCC) with two “good” indicators, true positive rate and false positive rate."
  },
  {
    "objectID": "Week8.html",
    "href": "Week8.html",
    "title": "8  Week8 - Temperature and Policy",
    "section": "",
    "text": "The “policy” section occupies two weeks, mainly trying to introduce how to fit EO data workflow into current policies. To do this, you have to identify the gaps, e.g., that between the overarching global policies, metropolitan plans and local plans. Or, the gap within policies like missing locations in the Singapore one."
  },
  {
    "objectID": "Week8.html#summary",
    "href": "Week8.html#summary",
    "title": "8  Week8 - Temperature and Policy",
    "section": "8.1 Summary",
    "text": "8.1 Summary\n\n8.1.1 Urban Heating Islands (UHI) problem and plans\n\n8.1.1.1 Causes\nUrban areas have comparatively higher atmospheric and surface temperatures than surrounding rural areas, mainly due to\n\nMore dark surfaces that retain heat\nLess vegetation that cools the environment\n\nAlso, there are other contributors to the heat:\n\n\n\nContributor\nCorrelation with UHI phenomenon\n\n\n\n\nSky View Factor (SVF)\nPositive\n\n\nAir speed\nNegative\n\n\nHeavy cloud cover\nPositive\n\n\nCyclic solar radiation\nPositive\n\n\nBuilding material type\nVaries\n\n\nAnthropogenic energy\nPositive\n\n\n\n\n\n8.1.1.2 Cost\nThe cost of the Urban Heat Island can be divided into social, environmental, and economic costs.\n\n\n\n\n\n\n\nType of Cost\nExamples\n\n\n\n\nSocial Costs\nPopulation-adjusted excess mortality rates, heat-related deaths\n\n\nEnvironmental Costs\nIncrease in fossil fuel usage\n\n\nEconomic Costs\nLoss of Gross Domestic Product (GDP)\n\n\n\nFor instance, under a low greenhouse gas scenario, the percent GDP lost from UHI is estimated to be 0.71% in 2050.\n\n\n8.1.1.3 Plans\n\n8.1.1.3.1 Global\n\n\n8.1.1.3.2 Local\n\n\n\n\n\n\n\n\nCity\nInitiatives\nTechnology\n\n\n\n\nSingapore\nGreen buildings\nUrban greenery\n\n\nMedellin\nGreen Corridors\nUrban greenery\n\n\nSydney\nTurn Down the Heat Strategy and Action Plan\nReflective roofs/pavements/sidewalks, Cool roads trial in Western Sydney"
  },
  {
    "objectID": "Week8.html#application",
    "href": "Week8.html#application",
    "title": "8  Week8 - Temperature and Policy",
    "section": "8.2 Application",
    "text": "8.2 Application\nMacLachlan et al. (2021) The document outlines a sub-city urban planning modeling approach using open-source tools to measure and monitor localised urban heat island (UHI) mitigation targets. The methodology involves comparing temperature dynamics of low- and high-density census areas using Earth observation data and determining optimal placement of greening elements in proposed plans using a data-driven model. The document concludes that this approach can be universally integrated into urban planning regulation frameworks to mitigate the localized UHI effect and ensure long-term city sustainability. Also it discusses the impact of low population density on housing in Perth, Australia, and the resulting need for strategic land zonation and sustainability targets. #### Why Data-driven approach\n\n8.2.1 Policy limitations\n\nlacking specificities for combating adverse temperature effects at the local level (sub-city), therefore not planning practicality\nno consistency in planning implementation methodologies or steps for assessing progress toward UHI reduction targets\nlackage of empirical evidence for optimizing UHI mitigation strategies\n\n\n\n8.2.2 Data to drive the new approach\n\nEarth Observation (EO) data can be processed to identify (un)sustainable urban development through aerial assessments of land cover change\ntemperature\nelevation\n\n\n\n8.2.3 Advantages for the Data-driven approach\n\nEO data can produce consistent information necessary for restricting unsustainable development\nmonitor UHI effects based on associated land-temperature dynamics\nAssessment at finer spatial scales (e.g., block subdivisions)"
  },
  {
    "objectID": "Week8.html#methodology",
    "href": "Week8.html#methodology",
    "title": "8  Week8 - Temperature and Policy",
    "section": "8.3 Methodology",
    "text": "8.3 Methodology\n\n\n8.3.1 Temperature Modeling\n\n\nModeled temperature every 3 hours using SOLWEIG model between 2008 and 2010\n\n\n\nInputs generated from meteorological data, land cover, building DSM, ground DTM, and vegetation canopy REM in QGIS\n\n\n\nSVF computed from vegetation canopy REM, building DSM, and ground DTM using UMEP plugin\n\n\n\nBuilding wall heights and aspect generated from DSM and DTM using UMEP plugin\n\n\n\n\n8.3.2 Data-Driven Tree Placement\n\n\nSite selected for modeling temperature in the City of Fremantle\n\n\n\nThree scenarios processed: current urban footprint, proposed changes, and proposed redevelopment with no trees\n\n\n\nHighest temperatures identified and used to redesign tree placement\n\n\n\n15 trees distributed according to original design aspects\n\n\n\nUpdated vegetation canopy REM reflected new tree locations\n\n\n\nAnalysis re-run to compare temperature across redevelopment site\n\n\n\nModeled all scenarios accounting for influence of neighboring landscape features"
  },
  {
    "objectID": "Week8.html#result",
    "href": "Week8.html#result",
    "title": "8  Week8 - Temperature and Policy",
    "section": "8.4 Result",
    "text": "8.4 Result\nTable 1: Results of assessments of urban design factors on UHI effect in Perth.\n\n\n\nFactors assessed\nEffect on UHI effect\n\n\n\n\nVegetation cover\nNegative correlation with UHI effect\n\n\nCanopy cover\nNegative correlation with UHI effect\n\n\nBuilding density\nPositive correlation with UHI effect\n\n\nBuilding height\nPositive correlation with UHI effect\n\n\nAlbedo\nNegative correlation with UHI effect\n\n\nLand use\nNegative correlation with UHI effect\n\n\nUrban sprawl\nPositive correlation with UHI effect\n\n\n\nTable 2: Total population and population density per 0.1 km2 between 2011 and 2016 for Subiaco and Currambine SA1s as defined by the ABS.\n\n\n\n\n\n\n\n\n\n\nSA1\nTotal Population (2011)\nTotal Population (2016)\nPopulation Density per 0.1 km2 (2011)\nPopulation Density per 0.1 km2 (2016)\n\n\n\n\nSubiaco\nN/A\nN/A\n325\n712\n\n\nCurrambine\nN/A\nN/A\n310\n324"
  },
  {
    "objectID": "Week8.html#reflection",
    "href": "Week8.html#reflection",
    "title": "8  Week8 - Temperature and Policy",
    "section": "8.5 Reflection",
    "text": "8.5 Reflection\n\nCase study: Superblocks, Barcelona\n\nBasically about pedestrian economy. Though there have been many retail modes like KFC and other American fast food, the experience from Europe tells that economy vitality can have a boost with pedestrian-dominated areas. See Barcelona\n\n\n\n\nMacLachlan, Andrew, Eloise Biggs, Gareth Roberts, and Bryan Boruff. 2021. “Sustainable City Planning: A Data-Driven Approach for Mitigating Urban Heat.” Frontiers in Built Environment 6. https://www.frontiersin.org/articles/10.3389/fbuil.2020.519599."
  },
  {
    "objectID": "Week9.html",
    "href": "Week9.html",
    "title": "9  Week 9 - Synthetic Aperture Radar (SAR) data",
    "section": "",
    "text": "This week addresses problems in\n\nThe object of using Synthetic Aperture Radar (SAR)\n\nDetecting changes in the Earth’s surface over time\n\nThe advantages of SAR for change detection\n\nsee through clouds\nhigh temporal resolution\n\nTechniques for change detection with SAR?\n\nratio\nlog ratios between two images\nt-tests\n\nfused with other data?\nYes, with optical data using techniques such as\n\nprincipal component analysis\nobject-based image analysis\nintensity fusion\n\nApplications?\n\nmonitoring land use changes\ndetecting deforestation\nidentifying urban growth pattern\n\n\n\n\n\n\nDefinition: Synthetic Aperture Radar (SAR) is a type of radar that uses microwave signals to create high-resolution images of the Earth’s surface.\nAdvantages:\n\nOperates in all weather conditions\nPenetrates through clouds and vegetation cover.\n\nLimitations\n\nSensitive to surface roughness; limited spatial resolution.\n\nProcessing Techniques\n\nInterferometry: combines multiple SAR images to create 3D maps of the Earth’s surface.\nPolarimetry: analyzes the polarization properties of reflected signals to extract additional information about surface features.\n\nThe relationship between different surfaces and their sensitivity to polarizations in SAR data\n\n\n\n\n\n\n\n\nSurface Type\nScattering Mechanism\nMost Sensitive Polarization\n\n\n\n\nRough (bare earth)\nRough Scattering\nVertical-Vertical (VV)\n\n\nLeaves\nVolume Scattering\nVertical-Horizontal (VH) or Horizontal-Vertical (HV)\n\n\nTrees / Buildings\nDouble Bounce\nHorizontal-Horizontal (HH)\n\n\n\nApplications:\n\nEnvironmental monitoring, disaster response, urban planning, military surveillance, and more.\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nKey Points\n\n\n\n\nAdvantages of SAR for Change Detection\nCan see through clouds unlike optical sensors; high temporal resolution.\n\n\nChange Detection Techniques\nRatio or log ratios between two images; t-tests; standard deviation.\n\n\nFusion of SAR and Optical Data\nPrincipal component analysis; object-based image analysis; intensity fusion.\n\n\nApplications of Change Detection with SAR\nMonitoring land use changes, detecting deforestation, identifying urban growth patterns, and more.\n\n\n\n\n\n\nResolution, accuracy, real-time-ness and data scale in SAR might see advancements.\n\nImproved resolution and accuracy:\n\nUrban planning: Improved resolution and accuracy can influence local zoning regulations and urban growth management by providing detailed information on land use changes and the built environment. For example, high-resolution SAR data can be used to assess the effectiveness of urban containment policies or to identify areas where infrastructure investments are needed.\n\nData processing capabilities\n\nDisaster response: The ability to process larger datasets and monitor Earth’s surface in near real-time can inform global policies regarding disaster management, such as the Sendai Framework for Disaster Risk Reduction. Boosted rapidness and data capability of SAR can better response to natural disasters, like earthquakes or hurricanes. This allows for quicker design of resources deployment strategy, and more comprehensive information in managing affected areas.\n\nNew SAR applications\n\nAgricultural: Enhanced SAR facilitates its use in change detection and monitoring. This will support policies like the European Union’s Common Agricultural Policy (CAP), by providing data on crop health, irrigation needs, and land use changes.\nForestry (deforestation and reforestation tracking)\nDisaster response (flood and landslide monitoring)\nEnvironmental management: SAR data can inform policies related to wetland and coastal zone management, such as the Ramsar Convention on Wetlands and the United Nations Convention on the Law of the Sea (UNCLOS). By monitoring changes in these sensitive areas, policymakers can evaluate the effectiveness of existing regulations and develop new strategies to protect vital ecosystems.\n\nMore matured machine learning and artificial intelligence:\n\nAdvanced algorithms that are yet to be developed or need further maturity for SAR data analysis could include:\n\n\n\n\n\n\n\n\n\nAlgorithm\nPros\nCons\n\n\n\n\nImproved Unsupervised Change Detection Algorithms\n- No need for labeled training data. - Can discover unknown or unexpected changes.\n- May struggle with complex or subtle changes. - Can be sensitive to noise and variations in the data.\n\n\nMulti-Modal Fusion Algorithms\n- Combines SAR data with other sources (e.g., optical, hyperspectral) for better feature identification. - Can exploit the complementary strengths of different data types.\n- Requires data synchronization and registration, which can be challenging. - May involve increased complexity and computational cost.\n\n\nGraph-based Change Detection Algorithms\n- Can model complex spatial relationships between features. - Robust against noise and speckle effects in SAR data.\n- Computationally expensive, especially for large-scale datasets. - May require tuning of hyperparameters.\n\n\n\n\nImproved overall accuracy of change detection can support climate change adaptation efforts at both local and global levels, including the United Nations Framework Convention on Climate Change (UNFCCC) and the Paris Agreement. Predictive models based on SAR data can help policymakers identify areas at risk of flooding, coastal erosion, or other climate-related impacts, enabling the targeted adaptation strategies.\n\n\nAdvancements in SAR technology, combined with the integration of machine learning and AI, will enhance change detection capabilities, enabling new analysis avenues in sectors including agriculture, forestry, disaster management, and environmental protection, ultimately influencing policymaking and promoting more informed decision-making processes."
  },
  {
    "objectID": "Week9.html#application",
    "href": "Week9.html#application",
    "title": "9  Week 9 - Synthetic Aperture Radar (SAR) data",
    "section": "9.2 Application",
    "text": "9.2 Application\nThis week I chose to explore the application of SAR in wetland classification and real-time monitoring, as well as its future advancement, in the context of a literature recommended by the module webpage (Dabboor and Brisco 2019).\nWetland acts as a kidney to Earth reminding one of the old positivist tradition in French philosophy, taking abstract structures as organisms. The chance to explore this wholist view with an analytic approach is thrilling, as these two paradigms that seem to be falling in a prevailing antithesis can actually sparkle inspiration and exhibit harmony of inclusion of each other.\n\n9.2.1 Wetland classification\nWetland classification has been a daunting task utilising traditional air photo and filed visits. Ever since the launch of ERTS in 1972, this task has been expecting the evolution of methods through applying Earth Observation data.\nThe SOTA of this tasks, as implied in the literature, has been an Object-based classification:\n\nCombining multi-source data: optical and SAR\nAnalysing using a machine-learning classification model\nIncorporating a Digital Elevation Model (DEM)\nAims at identifying terrain suitability for wetlands and surface water\n\nSee @ for a workflow\nThis method achieves over 90% of accuracy and is useful in that its core method utilises SAR data’s feature of “seeing under the water” to better identify flooded vegetation class.\nHow did this amazing feature come about? Is that an emerging effect due to unprecedented combination of data? Or is it determined by the characteristics innate to SAR data?\nThe answer is the latter: The flooded vegetation tends to produce a double bounce scattering mechanism, which increases the intensity of the backscatter, making HH polarization to be the best for this due to the enhanced penetration in vegetation (Jahncke2018MappingWI?).\n\n\n\nCredit: Dabboor and Brisco (2019)\n\n\nThis workflow utilising different scattering effect between water and vegetation is one of the main contributors to the SOTA performance, laying the ground for further development and iteration on this method: various angle-choosing strategies, machine-learning algorithm combinations, etc.\n\n\n9.2.2 Future Advancement\n\nResolution in time and space can see chance of enhancement. Spaceborne SAR remote sensing technology being the essential tool for effective wetland observation, its improvement can be expected to reflect on enhancement of wetland observation in temporal and spatial resolution, e.g., the RCM is expected to provide SAR imagery in a spatial resolution ranging from 1 m to 100 m, in a revisit time of only 4 days (Dabboor and Brisco 2019).\n\nThis can better our understanding of climate change in wetlands and water quality, allowing ecosystem managers and decision makers to have sufficient information regarding wetland preservation\n\nMore sensors with more data forms, as well as improved data quality can be anticipated in the future. The integration of SAR imagery with optical and topographic data from multiple sensors was shown in Dubeau et al. (2017) to be necessary for improved wetland mapping and classification during the growing season.\n\nHowever, the integration of SAR imagery and LiDAR data did not improve significantly the classification accuracy of wetland in Millard and Richardson (2013).\n\nThe effectiveness of machine learning algorithms for automated wetland classification can expect further development. E.g., Graph-based Change Detection Algorithms can model complex spatial relationships between features and is robust against noise and speckle effects in SAR data.\n\nThis shift toward the automated machine learning algorithms comes to fulfill the requirement for operational wetland monitoring systems.\n\n\n\n\nDabboor, Mohammed, and Brian Brisco. 2019. “Wetland Monitoring and Mapping Using Synthetic Aperture Radar.” In Wetlands Management - Assessing Risk and Sustainable Solutions, edited by Didem Gökçe. IntechOpen. https://doi.org/10.5772/intechopen.80224.\n\n\nDubeau, Pierre, Douglas J. King, Dikaso Gojamme Unbushe, and Lisa-Maria Rebelo. 2017. “Mapping the Dabus Wetlands, Ethiopia, Using Random Forest Classification of Landsat, PALSAR and Topographic Data.” Remote Sensing 9 (10). https://doi.org/10.3390/rs9101056.\n\n\nMillard, Koreen, and Murray Richardson. 2013. “Wetland Mapping with LiDAR Derivatives, SAR Polarimetric Decompositions, and LiDAR–SAR Fusion Using a Random Forest Classifier.” Canadian Journal of Remote Sensing 39 (4): 290–307. https://doi.org/10.5589/m13-038."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10  Summary",
    "section": "",
    "text": "1 + 1\n\n2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Butcher, Ginger. 2016. Tour of the Electromagnetic Spectrum.\nThird edition. Washington, DC: National Aeronautics; Space\nAdministration.\n\n\nDabboor, Mohammed, and Brian Brisco. 2019. “Wetland\nMonitoring and Mapping Using\nSynthetic Aperture Radar.”\nIn Wetlands Management - Assessing\nRisk and Sustainable\nSolutions, edited by Didem Gökçe. IntechOpen. https://doi.org/10.5772/intechopen.80224.\n\n\nDubeau, Pierre, Douglas J. King, Dikaso Gojamme Unbushe, and Lisa-Maria\nRebelo. 2017. “Mapping the Dabus\nWetlands, Ethiopia, Using\nRandom Forest Classification of\nLandsat, PALSAR and Topographic\nData.” Remote Sensing 9 (10). https://doi.org/10.3390/rs9101056.\n\n\nGoogle. 2023a. “Machine Learning in\nEarth Engine  Google\nEarth Engine.” Google\nDevelopers. https://developers.google.com/earth-engine/guides/machine-learning.\n\n\n———. 2023b. “Reducer Overview \nGoogle Earth Engine.”\nGoogle Developers. https://developers.google.com/earth-engine/guides/reducers_intro.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David\nThau, and Rebecca Moore. 2017. “Google Earth\nEngine: Planetary-Scale Geospatial Analysis\nfor Everyone.” Remote Sensing of Environment, Big\nRemotely Sensed Data: Tools,\napplications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nJensen, J. Robert. 1986. “Introductory Digital Image Processing: A\nRemote Sensing Perspective.” In.\n\n\nMacLachlan, Andrew, Eloise Biggs, Gareth Roberts, and Bryan Boruff.\n2021. “Sustainable City Planning:\nA Data-Driven\nApproach for Mitigating Urban\nHeat.” Frontiers in Built Environment 6. https://www.frontiersin.org/articles/10.3389/fbuil.2020.519599.\n\n\nMillard, Koreen, and Murray Richardson. 2013. “Wetland Mapping\nwith LiDAR Derivatives, SAR Polarimetric\nDecompositions, and LiDAR–SAR Fusion Using a\nRandom Forest Classifier.” Canadian Journal of Remote\nSensing 39 (4): 290–307. https://doi.org/10.5589/m13-038.\n\n\nMoore, R., and M. Hansen. 2011. “Google Earth\nEngine: A New Cloud-Computing Platform for Global-Scale\nEarth Observation Data and Analysis.” AGU Fall Meeting\nAbstracts, December, 02.\n\n\nNair, Rahul, P. J. Paul, and K. Poulose Jacob. 2018. “Automated\nImage Annotation: A Survey.” Journal of Imaging 4 (3):\n37.\n\n\nRobison, Keela. 2018. “The Future of Image Annotation: Human in\nthe Loop.” https://lionbridge.ai/articles/the-future-of-image-annotation-human-in-the-loop/.\n\n\nSaad El Imanni, Hajar, Abderrazak El Harti, El Mostafa Bachaoui, Hicham\nMouncif, Fatine Eddassouqui, Mohamed Achraf Hasnai, and Moulay Ismail\nZinelabidine. 2023. “Multispectral UAV Data for\nDetection of Weeds in a Citrus Farm Using Machine Learning and\nGoogle Earth Engine:\nCase Study of Morocco.” Remote\nSensing Applications: Society and Environment 30 (April): 100941.\nhttps://doi.org/10.1016/j.rsase.2023.100941.\n\n\nSchulte to Bühne, Henrike, and Nathalie Pettorelli. 2018. “Better\nTogether: Integrating and Fusing Multispectral and Radar\nSatellite Imagery to Inform Biodiversity Monitoring, Ecological Research\nand Conservation Science.” Methods in Ecology and\nEvolution 9 (4): 849–65. https://doi.org/10.1111/2041-210X.12942."
  }
]